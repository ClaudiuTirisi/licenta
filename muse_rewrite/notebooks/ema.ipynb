{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace_copy(target_tensor, source_tensor):\n",
    "  target_tensor.copy_(source_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace_lerp(target_tensor, source_tensor, weight):\n",
    "  target_tensor.lerp_(source_tensor, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA(Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      model,\n",
    "      beta: 0.999,\n",
    "      update_after_step = 100,\n",
    "      update_every = 10,\n",
    "      inv_gamma = 1.0,\n",
    "      power = 2 / 3,\n",
    "      min_value = 0.0,\n",
    "      param_or_buffer_names_no_ema = set(),\n",
    "      ignore_names = set(),\n",
    "      ignore_startswith_names = set(),\n",
    "      include_online_model = True,\n",
    "      allow_different_devices = False,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.beta = beta\n",
    "    self.is_frozen = beta == 1\n",
    "\n",
    "    self.include_online_model = include_online_model\n",
    "\n",
    "    self.online_model = model\n",
    "    \n",
    "    try:\n",
    "      self.ema_model = deepcopy(model)\n",
    "    except Exception as e:\n",
    "      print(f'Error while trying to deepcopy model for EMA: {e}')\n",
    "      exit()\n",
    "\n",
    "    self.ema_model.requires_grad_(False)\n",
    "\n",
    "    self.parameter_names = {name for name, param in self.ema_model.named_parameters() if torch.is_floating_point(param) or torch.is_complex(param)}\n",
    "    self.buffer_names = {name for name, buffer in self.ema_model.named_buffers() if torch.is_floating_point(buffer) or torch.is_complex(buffer)}\n",
    "    \n",
    "    self.update_every = update_every\n",
    "    self.update_after_step = update_after_step\n",
    "\n",
    "    self.inv_gamma = inv_gamma\n",
    "    self.power = power\n",
    "    self.min_value = min_value\n",
    "\n",
    "    self.param_or_buffer_names_no_ema = param_or_buffer_names_no_ema\n",
    "\n",
    "    self.ignore_names = ignore_names\n",
    "    self.ignore_startswith_names = ignore_startswith_names\n",
    "\n",
    "    self.allow_different_devices = allow_different_devices\n",
    "\n",
    "    self.register_buffer('initted', torch.tensor(False))\n",
    "    self.register_buffer('step', torch.tensor(0))\n",
    "\n",
    "  @property\n",
    "  def model(self):\n",
    "    return self.online_model\n",
    "  \n",
    "  def eval(self):\n",
    "    return self.ema_model.eval()\n",
    "\n",
    "  def restore_ema_model_device(self):\n",
    "    device = self.initted.device\n",
    "    self.ema_model.to(device)\n",
    "\n",
    "  def get_params_iter(self, model):\n",
    "    for name, param in model.named_parameters():\n",
    "      if name not in self.parameter_names:\n",
    "        continue\n",
    "      yield name, param\n",
    "\n",
    "  def buffer_iter(self, model):\n",
    "    for name, buffer in model.named_buffers():\n",
    "      if name not in self.buffer_names:\n",
    "        continue\n",
    "      yield name, buffer\n",
    "\n",
    "  def copy_params_from_model_to_ema(self):\n",
    "    copy = self.inplace_copy\n",
    "    for (_, ma_params), (_, current_params) in zip(self.get_params_iter(self.ema_model), self.get_params_iter(self.model)):\n",
    "      copy(ma_params.data, current_params.data)\n",
    "\n",
    "    for (_, ma_buffers), (_, current_buffers) in zip(self.get_buffers_iter(self.ema_model), self.get_buffers_iter(self.model)):\n",
    "      copy(ma_buffers.data, current_buffers.data)\n",
    "\n",
    "  def get_current_decay(self):\n",
    "    \"\"\"\n",
    "    decay = (-epoch)^(-2/3)\n",
    "    starts at 1 at quickly decreases\n",
    "    \"\"\"\n",
    "    epoch = (self.step - self.update_after_step - 1).clamp(min = 0.)\n",
    "    \n",
    "    value = 1 - (1 + epoch / self.inv_gamma) ** (-self.power)\n",
    "\n",
    "    if epoch.item() <= 0:\n",
    "      return 0.\n",
    "    \n",
    "    return value.clamp(min = self.min_value, max = self.beta).item()\n",
    "\n",
    "  def update(self):\n",
    "    step = self.step.item()\n",
    "    self.step += 1\n",
    "\n",
    "    if (step % self.update_every) != 0:\n",
    "      return\n",
    "    \n",
    "    # if we haven't yet updated EMA params, initialized them now\n",
    "    if step <= self.update_after_step:\n",
    "      self.copy_params_from_model_to_ema()\n",
    "\n",
    "    # same as above\n",
    "    # perhaps the use case is, loading from a saved model? \n",
    "    # but that should save step too\n",
    "    if not self.initted.item():\n",
    "      self.copy_params_from_model_to_ema()\n",
    "      self.initted.data.copy_(torch.tensor(True))\n",
    "\n",
    "    self.update_moving_average(self.ema_model, self.model)\n",
    "\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def update_moving_average(self, ma_model, current_model):\n",
    "    if self.is_frozen:\n",
    "      return\n",
    "    \n",
    "    copy, lerp = self.inplace_copy, self.inplace_lerp\n",
    "    current_decay = self.get_current_decay()\n",
    "\n",
    "    for (name, current_params), (_, ma_params) in zip(self.get_params_iter(current_model), self.get_params_iter(ma_model)):\n",
    "      lerp(ma_params.data, current_params.data, 1. - current_decay)\n",
    "\n",
    "    for (name, current_buffer), (_, ma_buffer) in zip(self.get_buffers_iter(current_model), self.get_buffers_iter(ma_model)):\n",
    "      lerp(ma_buffer.data, current_buffer.data)\n",
    "\n",
    "  def __call__(self, *args, **kwargs):\n",
    "    return self.ema_model(*args, **kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
