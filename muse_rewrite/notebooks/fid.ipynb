{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calculates the Frechet Inception Distance (FID) to evalulate GANs\\n\\nThe FID metric calculates the distance between two distributions of images.\\nTypically, we have summary statistics (mean & covariance matrix) of one\\nof these distributions, while the 2nd distribution is given by a GAN.\\n\\nWhen run as a stand-alone program, it compares the distribution of\\nimages that are stored as PNG/JPEG at a specified location with a\\ndistribution given by summary statistics (in pickle format).\\n\\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\\nthe pool_3 layer of the inception net for generated samples and real world\\nsamples respectively.\\n\\nSee --help to see further details.\\n\\nCode apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\\nof Tensorflow\\n\\nCopyright 2018 Institute of Bioinformatics, JKU Linz\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing, software\\ndistributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Calculates the Frechet Inception Distance (FID) to evalulate GANs\n",
    "\n",
    "The FID metric calculates the distance between two distributions of images.\n",
    "Typically, we have summary statistics (mean & covariance matrix) of one\n",
    "of these distributions, while the 2nd distribution is given by a GAN.\n",
    "\n",
    "When run as a stand-alone program, it compares the distribution of\n",
    "images that are stored as PNG/JPEG at a specified location with a\n",
    "distribution given by summary statistics (in pickle format).\n",
    "\n",
    "The FID is calculated by assuming that X_1 and X_2 are the activations of\n",
    "the pool_3 layer of the inception net for generated samples and real world\n",
    "samples respectively.\n",
    "\n",
    "See --help to see further details.\n",
    "\n",
    "Code apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\n",
    "of Tensorflow\n",
    "\n",
    "Copyright 2018 Institute of Bioinformatics, JKU Linz\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\n",
    " \n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as TF\n",
    "from PIL import Image\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from inception import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--batch-size'], dest='batch_size', nargs=None, const=None, default=50, type=<class 'int'>, choices=None, help='Batch size to use', metavar=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser(formatter_class = ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument(\"--batch-size\", type=int, default=50, help=\"Batch size to use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num-workers'], dest='num_workers', nargs=None, const=None, default=None, type=<class 'int'>, choices=None, help='Number of processes to use for data loading. Default to `min(8, num_cpus)`', metavar=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\n",
    "  \"--num-workers\",\n",
    "  type=int,\n",
    "  help=(\n",
    "    \"Number of processes to use for data loading. \"\n",
    "    \"Default to `min(8, num_cpus)`\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--device'], dest='device', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='Device to use. Like cuda, cuda:0 or cpu', metavar=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\n",
    "  \"--device\",\n",
    "  type=str,\n",
    "  default=None,\n",
    "  help = \"Device to use. Like cuda, cuda:0 or cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--dims'], dest='dims', nargs=None, const=None, default=2048, type=<class 'int'>, choices=[64, 192, 768, 2048], help='Dimensionality of Inception features (channel count) to use. By default, uses pool3 features', metavar=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\n",
    "  \"--dims\",\n",
    "  type=int,\n",
    "  default = 2048,\n",
    "  choices = list(InceptionV3.BLOCK_INDEX_BY_DIM),\n",
    "  help = (\n",
    "    \"Dimensionality of Inception features (channel count) to use. \"\n",
    "    \"By default, uses pool3 features\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreTrueAction(option_strings=['--save-stats'], dest='save_stats', nargs=0, const=True, default=False, type=None, choices=None, help='Generate an npz archive from a directory of samples. The first path is used as input and the second as output.', metavar=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\n",
    "  \"--save-stats\",\n",
    "  action = \"store_true\",\n",
    "  help = (\n",
    "    \"Generate an npz archive from a directory of samples. \"\n",
    "    \"The first path is used as input and the second as output.\"\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=[], dest='path', nargs=2, const=None, default=None, type=<class 'str'>, choices=None, help='Paths to the generated images or to .npz statistic files', metavar=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument(\n",
    "  \"path\",\n",
    "  type=str,\n",
    "  nargs=2,\n",
    "  help=(\"Paths to the generated images or to .npz statistic files\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTENSIONS= {\"bmp\", \"jpg\", \"jpeg\", \"pgm\", \"png\", \"ppm\", \"tif\", \"tiff\", \"webp\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePathDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, files, transforms = None):\n",
    "    self.files = files\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.files)\n",
    "\n",
    "  # appears to return PIL images, unless explicitly converted to tensor\n",
    "  def __getitem__(self, i):\n",
    "    path = self.files[i]\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    if self.transforms is not None:\n",
    "      img = self.transforms(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(files, model, batch_size = 50, dims = 2048, device = \"cpu\", num_workers = 1):\n",
    "  \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- files       : List of image files paths\n",
    "    -- model       : Instance of inception model\n",
    "    -- batch_size  : Batch size of images for the model to process at once.\n",
    "                     Make sure that the number of samples is a multiple of\n",
    "                     the batch size, otherwise some samples are ignored. This\n",
    "                     behavior is retained to match the original FID score\n",
    "                     implementation.\n",
    "    -- dims        : Dimensionality of features returned by Inception\n",
    "    -- device      : Device to run calculations\n",
    "    -- num_workers : Number of parallel dataloader workers\n",
    "\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, dims) that contains the\n",
    "       activations of the given tensor when feeding inception with the\n",
    "       query tensor.\n",
    "    \"\"\"\n",
    "  \n",
    "  model.eval()\n",
    "  if batch_size > len(files):\n",
    "    print(\"Warning: batch size is bigger than data size. Setting batch size to data size\")\n",
    "    batch_size = len(files)\n",
    "\n",
    "  dataset = ImagePathDataset(files, transforms = TF.ToTensor())\n",
    "  dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    drop_last = False,\n",
    "    num_workers = num_workers\n",
    "  )\n",
    "\n",
    "  pred_arr = np.empty((len(files), dims))\n",
    "\n",
    "  start_idx = 0\n",
    "  for batch in tqdm(dataloader):\n",
    "    batch = batch.to(device)\n",
    "    with torch.no_grad():\n",
    "      pred = model(batch)[0]\n",
    "\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "      pred = adaptive_avg_pool2d(pred, output_size = (1, 1))\n",
    "\n",
    "    pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
    "    pred_arr[start_idx : start_idx + pred.shape[0]] = pred\n",
    "\n",
    "    start_idx = start_idx + pred.shape[0]\n",
    "\n",
    "  return pred_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechest_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "\n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1   : Numpy array containing the activations of a layer of the\n",
    "               inception net (like returned by the function 'get_predictions')\n",
    "               for generated samples.\n",
    "        Claudiu edit: the description of mu1 seems wrong. The function is called as if mu1 was the sample mean of generated sampels\n",
    "    -- mu2   : The sample mean over activations, precalculated on an\n",
    "               representative data set.\n",
    "    -- sigma1: The covariance matrix over activations for generated samples.\n",
    "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
    "               representative data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "    \n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert(\n",
    "        mu1.shape == mu2.shape\n",
    "    ), \"Training and test mean vectors have different lengths\"\n",
    "\n",
    "    assert(\n",
    "        sigma1.shape == sigma2.shape\n",
    "    ), \"Trainig and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = (\n",
    "            \"fid calculation produces singular product; \"\n",
    "            f'adding {eps} to diagonal of cov estimates'\n",
    "        )\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    # cov matrix may contain imaginary components due to numerical errors\n",
    "    # if the imaginary parts of the diagonal values are close enough to 0\n",
    "    # then we ignore this by taking the real part\n",
    "    if np.iscomplexob(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(f\"Imaginary component {m}\")\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2*tr_covmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(\n",
    "    files, model, batch_size = 50, dims = 2048, device = \"cpu\", num_workers = 1\n",
    "):\n",
    "  \"\"\"Calculation of the statistics used by the FID.\n",
    "  Params:\n",
    "  -- files       : List of image files paths\n",
    "  -- model       : Instance of inception model\n",
    "  -- batch_size  : The images numpy array is split into batches with\n",
    "                    batch size batch_size. A reasonable batch size\n",
    "                    depends on the hardware.\n",
    "  -- dims        : Dimensionality of features returned by Inception\n",
    "  -- device      : Device to run calculations\n",
    "  -- num_workers : Number of parallel dataloader workers\n",
    "\n",
    "  Returns:\n",
    "  -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "              the inception model.\n",
    "  -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "              the inception model.\n",
    "  \"\"\"\n",
    "\n",
    "  act = get_activations(files, model, batch_size, dims, device, num_workers)\n",
    "  mu = np.mean(act, axis = 0)\n",
    "  sigma = np.cov(act, rowvar = False)\n",
    "  return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.random.rand(10, 20), axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_of_path(path, model, batch_size, dims, device, num_workers = 1):\n",
    "  if path.endswith(\".npz\"):\n",
    "    with np.load(path) as f:\n",
    "      m, s = f[\"mu\"][:], f[\"sigma\"][:]\n",
    "  else:\n",
    "    path = pathlib.Path(path)\n",
    "    files = sorted(\n",
    "      [file for ext in IMAGE_EXTENSIONS for file in path.glob(f'*.{ext}')]\n",
    "    )\n",
    "    m, s = calculate_activation_statistics(\n",
    "      files, model, batch_size, dims, device, num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid_given_paths(paths, batch_size, device, dims, num_workers = 1):\n",
    "  for p in paths:\n",
    "    if not os.path.exists(p):\n",
    "      raise RuntimeError(f'Invalid path {p}')\n",
    "\n",
    "  block_id = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "  model = InceptionV3(block_id).to(device)\n",
    "\n",
    "  m1, s1 = compute_statistics_of_path(\n",
    "    paths[0], model, batch_size, dims, device, num_workers\n",
    "  )\n",
    "\n",
    "  m2, s2 = compute_statistics_of_path(\n",
    "    paths[1], model, batch_size, dims, device, num_workers\n",
    "  )\n",
    "  fid_value = calculate_frechest_distance(m1, s1, m2, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fid_stats(paths, batch_size, device, dims, num_workers = 1):\n",
    "  if not os.path.exists(paths[0]):\n",
    "    raise RuntimeError(f'Invalid path: {paths[0]}')\n",
    "\n",
    "  if os.path.exists(paths[1]):\n",
    "    raise RuntimeError(f'Output file already exists: {paths[1]}')\n",
    "  \n",
    "  block_id = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "\n",
    "  model = InceptionV3(block_id).to(device)\n",
    "\n",
    "  m1, s1 = compute_statistics_of_path(\n",
    "    paths[0], model, batch_size, dims, device, num_workers\n",
    "  )\n",
    "\n",
    "  np.savez_compressed(paths[1], mu=m1, sigma=s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  args = parser.parse_args()\n",
    "  if args.device is None:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "  else:\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "  if args.num_workers is None:\n",
    "    try:\n",
    "      num_cpus = len(os.sched_get_affinity(0))\n",
    "    except AttributeError:\n",
    "      num_cpus = os.cpu_count()\n",
    "    num_workers = min(num_cpus, 8) if num_cpus is not None else 0\n",
    "  else:\n",
    "    num_workers = args.num_workers\n",
    "\n",
    "  if args.save_stats:\n",
    "    \"\"\"\n",
    "    Compute mean and cov matrix for images in args.path[0] and save them in the file at args.path[1]\n",
    "    \"\"\"\n",
    "    save_fid_stats(args.path, args.batch_size, device, args.dims, num_workers)\n",
    "    return\n",
    "  \n",
    "  \"\"\"\n",
    "  Compute FID between images found at args.path[0] and images found at args.path[1]\n",
    "  \"\"\"\n",
    "  fid_value = calculate_fid_given_paths(\n",
    "    args.path, args.batch_size, device, args.dims, num_workers\n",
    "  )\n",
    "  print(\"FID: \", fid_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size BATCH_SIZE]\n",
      "                             [--num-workers NUM_WORKERS] [--device DEVICE]\n",
      "                             [--dims {64,192,768,2048}] [--save-stats]\n",
      "                             path path\n",
      "ipykernel_launcher.py: error: the following arguments are required: path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/muse/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3513: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
