{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/muse/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from random import choice\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from functools import partial\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from vae import VQGanVAE\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from ema import EMA\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch_optimizer import Adafactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_or_no(question):\n",
    "  answer = input(f'{question} (y/n)')\n",
    "  return answer.lower() in ['yes', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(dl):\n",
    "  while True:\n",
    "    for data in dl:\n",
    "      yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accum_log(log, new_logs):\n",
    "  for key, new_value in new_logs.items():\n",
    "    old_value = log.get(key, 0.)\n",
    "    log[key] = old_value + new_value\n",
    "\n",
    "  return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "  def __init__(\n",
    "      self,\n",
    "      folder,\n",
    "      annotations_path,\n",
    "      image_size,\n",
    "      exts = ['jpg', 'jpeg', 'png']\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.image_size = image_size\n",
    "    \n",
    "    image_paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "\n",
    "    image_annotations = json.load(open(annotations_path))[\"annotations\"]\n",
    "\n",
    "    image_annotations_keyed = dict()\n",
    "    for annotation in image_annotations:\n",
    "      image_id = annotation[\"image_id\"]\n",
    "      caption = annotation[\"caption\"]\n",
    "      if image_id not in image_annotations_keyed:\n",
    "        image_annotations_keyed[image_id] = []\n",
    "      image_annotations_keyed[image_id].append(caption)\n",
    "\n",
    "    self.data = []\n",
    "    for path in tqdm(image_paths):\n",
    "      image_id = int(path.name.split(\".\")[0])\n",
    "      for text in image_annotations_keyed[image_id]:\n",
    "        image_data = {\n",
    "          \"path\": path,\n",
    "          \"texts\": text,\n",
    "        }\n",
    "        self.data.append(image_data)\n",
    "\n",
    "    print(f'Found {len(self.data)} training samples at {folder}')\n",
    "\n",
    "    self.transform = T.Compose([\n",
    "      T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "      T.Resize(image_size),\n",
    "      T.RandomHorizontalFlip(),\n",
    "      T.CenterCrop(image_size),\n",
    "      T.ToTensor()\n",
    "    ])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    path = self.data[index][\"path\"]\n",
    "    texts = self.data[index][\"texts\"]\n",
    "    img = Image.open(path)\n",
    "    return self.transform(img), texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskGitHighResTrainer(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      maskgit,\n",
    "      maskgit_lowres,\n",
    "      *,\n",
    "      image_folder,\n",
    "      caption_file,\n",
    "      num_train_steps,\n",
    "      batch_size,\n",
    "      image_size = 512,\n",
    "      image_size_lowres = 256,\n",
    "      lr = 1e-4,\n",
    "      weight_decay = 0.045,\n",
    "      grad_accum_every = 1,\n",
    "      save_results_every = 100,\n",
    "      save_model_every = 1000,\n",
    "      results_folder = \"./results-maskgit-highres\",\n",
    "      random_split_seed = 42, \n",
    "      valid_frac = 0.05,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.maskgit = maskgit\n",
    "    self.maskgit_lowres = maskgit_lowres\n",
    "    self.register_buffer('steps', torch.Tensor([0]))\n",
    "\n",
    "    self.accelerator = Accelerator()\n",
    "\n",
    "    self.num_train_steps = num_train_steps\n",
    "    self.batch_size = batch_size\n",
    "    self.grad_accum_every = grad_accum_every\n",
    "\n",
    "    all_parameters = set(maskgit.parameters())\n",
    "\n",
    "    self.optim = Adafactor(all_parameters, lr = lr, weight_decay = weight_decay)\n",
    "\n",
    "    self.image_size = 512\n",
    "    self.image_size_lowres = 256\n",
    "\n",
    "    # the highres 512x512 image will be resized for the lowres_transformer when necessary\n",
    "    # TODO:Figure out if it is faster to save the lowres_transformer results to disk and load\n",
    "    # them up when needing, like with the images themselves\n",
    "    self.ds = ImageTextDataset(image_folder, caption_file, image_size)\n",
    "\n",
    "    if valid_frac >0:\n",
    "      train_size = int((1 - valid_frac) * len(self.ds))\n",
    "      valid_size = len(self.ds) - train_size\n",
    "      self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n",
    "      print(f'Split dataset into {len(self.ds)} samples for training and {len(self.valid_ds)} samples for validating')\n",
    "    else:\n",
    "      self.valid_ds = self.ds\n",
    "      print(f'Training with shared training and validation dataset of {len(self.ds)} samples')\n",
    "\n",
    "    self.dl = DataLoader(\n",
    "      self.ds,\n",
    "      batch_size = batch_size,\n",
    "      shuffle = True\n",
    "    )\n",
    "\n",
    "    self.valid_dl = DataLoader(\n",
    "      self.valid_ds,\n",
    "      batch_size = batch_size,\n",
    "      shuffle = True\n",
    "    )\n",
    "\n",
    "    self.maskgit, self.optim, self.dl, self.valid_dl = self.accelerator.prepare(self.maskgit, self.optim, self.dl, self.valid_dl)\n",
    "\n",
    "    self.dl_iter = cycle(self.dl)\n",
    "    self.valid_dl_iter = cycle(self.valid_dl)\n",
    "\n",
    "    self.save_model_every = save_model_every\n",
    "    self.save_results_every = save_results_every\n",
    "\n",
    "    self.results_folder = Path(results_folder)\n",
    "\n",
    "    if len([*self.results_folder.glob(\"**/*\")]) > 0 and yes_or_no(\"Do you want to clear previous experiment checkpoints and results?\"):\n",
    "      rmtree(str(self.results_folder))\n",
    "\n",
    "    self.results_folder.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "  def save(self, path):\n",
    "    pkg = dict(\n",
    "      model = self.accelerator.get_state_dict(self.maskgit),\n",
    "      optim = self.optim.state_dict(),\n",
    "    )\n",
    "    torch.save(pkg, path)\n",
    "\n",
    "  def load(self, path):\n",
    "    path = Path(path)\n",
    "    assert path.exists()\n",
    "    pkg = torch.load(path)\n",
    "\n",
    "    maskgit = self.accelerator.unwrap_model(self.maskgit)\n",
    "    maskgit.load_state_dict(pkg['model'])\n",
    "\n",
    "    self.optim.load_state_dict(pkg['optim'])\n",
    "\n",
    "  @property\n",
    "  def device(self):\n",
    "    return self.accelerator.device\n",
    "  \n",
    "  def train_step(self):\n",
    "    device = self.device\n",
    "\n",
    "    steps = int(self.steps.item())\n",
    "\n",
    "    self.maskgit.train()\n",
    "\n",
    "    logs = {}\n",
    "    for _ in range(self.grad_accum_every):\n",
    "      img, texts = next(self.dl_iter)\n",
    "      img = img.to(device)\n",
    "\n",
    "      lowres_img = T.Resize(self.image_size_lowres)(img)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        lowres_ids = self.maskgit_lowres(\n",
    "          img,\n",
    "          texts,\n",
    "          return_indices = True\n",
    "        )\n",
    "\n",
    "      with self.accelerator.autocast():\n",
    "        loss = self.maskgit(\n",
    "          img,\n",
    "          texts,\n",
    "          cond_token_ids = lowres_ids\n",
    "        )\n",
    "        \n",
    "        self.accelerator.backward(loss / self.grad_accum_every)\n",
    "\n",
    "        accum_log(logs, {'loss': loss.item() / self.grad_accum_every})\n",
    "\n",
    "    self.optim.step()\n",
    "    self.optim.zero_grad()\n",
    "\n",
    "    print(f\"{steps}: transformer loss: {logs['loss']}\")\n",
    "\n",
    "    if not (steps % self.save_results_every) or steps == self.num_train_steps - 1:\n",
    "      \n",
    "      self.maskgit.eval()\n",
    "      valid_images, valid_texts = next(self.valid_dl_iter)\n",
    "      valid_images = valid_images.to(device)\n",
    "\n",
    "      output_images = self.maskgit.generate(valid_texts).detach().cpu().float().clamp(0., 1.)\n",
    "      grid = make_grid(output_images, nrow = 2, normalize = True, value_range = (0, 1))\n",
    "\n",
    "      save_image(grid, str(self.results_folder / f'{steps}.png'))\n",
    "      \n",
    "      with open(str(self.results_folder / f'{steps}.txt')) as f:\n",
    "        f.write(\";\".join(valid_texts))\n",
    "\n",
    "      print(f'{steps}: saving to {str(self.results_folder)}')\n",
    "\n",
    "      self.accelerator.wait_for_everyone()\n",
    "      if not (steps % self.save_model_every) or steps == self.num_train_steps - 1:\n",
    "        state_dict = self.accelerator.unwrap_model(self.maskgit).state_dict()\n",
    "        model_path = str(self.results_folder / f'maskgit.{steps}.pt')\n",
    "        self.accelerator.save(state_dict, model_path)\n",
    "\n",
    "        print(f'{steps}: saving model to {str(self.results_folder)}')\n",
    "\n",
    "    self.steps += 1\n",
    "    return logs\n",
    "\n",
    "  def train(self):\n",
    "    \n",
    "    while self.steps < self.num_train_steps:\n",
    "      logs = self.train_step()\n",
    "\n",
    "    print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
