{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import copy\n",
    "import math\n",
    "from math import sqrt\n",
    "from functools import partial, wraps\n",
    "\n",
    "from vector_quantize_pytorch import VectorQuantize as VQ, LFQ\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import torch.functional as F\n",
    "from torch.autograd import grad\n",
    "\n",
    "import torchvision\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_dict_by_key(cond, d):\n",
    "  \"\"\"\n",
    "  Splits dictionary d into 2 dictionaries such that\n",
    "  the first dictionary contains key-pair values for which the cond(key) = True\n",
    "  and the second dictionary contains everything else\n",
    "  \"\"\"\n",
    "  return_val = [dict(), dict()]\n",
    "  for key in d.keys():\n",
    "    match = bool(cond(key))\n",
    "    ind = int(not match)\n",
    "    return_val[ind][key] = d[key]\n",
    "  return (*return_val,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_begins_with(prefix, string_input):\n",
    "  return string_input.startswith(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_key_prefix(prefix, d):\n",
    "  \"\"\"\n",
    "  Returns 2 dicts, first has keys that start with the given prefix\n",
    "  and the second has the rest of the keys\n",
    "  \"\"\"\n",
    "  return group_dict_by_key(partial(string_begins_with, prefix), d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_prefix_and_trim(prefix, d):\n",
    "  \"\"\"\n",
    "  group_by_key_prefix but it removed the prefix from the resulting dict\n",
    "  \"\"\"\n",
    "  kwargs_with_prefix, kwargs = group_by_key_prefix(prefix, d)\n",
    "  kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n",
    "  return kwargs_without_prefix, kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(images, output, weight = 10):\n",
    "  \"\"\"\n",
    "  L2 gradient penalty that constraints the Discriminator function\n",
    "  to be 1-Lipschitz\n",
    "  grad computes the gradient of outputs with respect to inputs\n",
    "  assuming that outputs = L(f(inputs))\n",
    "  The chain rule says that ouputs' = L'(f(inputs)) * f'(inputs)\n",
    "  and grad requires that we pass the values of f'(inputs)\n",
    "  Since we are interested in penalizing the gradients of the discriminator\n",
    "  w.r.t to the inputs, we want the function f to be the identity\n",
    "  so we pass a tensor on 1s as its gradient\n",
    "  \"\"\"\n",
    "  gradients = grad(\n",
    "    outputs = output,\n",
    "    inputs = images,\n",
    "    grad_outputs = torch.ones(output.size(), device = images.device),\n",
    "    create_graph = True,\n",
    "    retain_graph = True,\n",
    "  )[0]\n",
    "\n",
    "  gradients = rearrange(gradients, 'b ... -> b (...)')\n",
    "  return weight * ((gradients.norm(2, dim = 1) - 1)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_layer_wrt_loss(loss, layer):\n",
    "  return grad(\n",
    "    outputs = loss,\n",
    "    inputs = layer,\n",
    "    grad_outputs = torch.ones_like(loss),\n",
    "    retain_graph = True\n",
    "  )[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLUResBlock(nn.Module):\n",
    "  def __init__(self, channels, groups = 16):\n",
    "    super().__init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Conv2d(channels, channels * 2, 3, padding = 1),\n",
    "      # first half of x * sigmoid(second half of x)\n",
    "      # halving alongside channel dimension\n",
    "      nn.GLU(dim = 1),\n",
    "      nn.GroupNorm(groups, channels),\n",
    "      nn.Conv2d(channels, channels * 2, 3, padding = 1),\n",
    "      nn.GLU(dim = 1),\n",
    "      nn.GroupNorm(groups, channels),\n",
    "      nn.Conv2d(channels, channels, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "  def __init__(self, channels, groups = 16):\n",
    "    super()._init__()\n",
    "    self.net = nn.Sequential(\n",
    "      nn.Conv2d(channels, channels, 3, padding = 1),\n",
    "      nn.GroupNorm(groups, channels),\n",
    "      nn.LeakyReLU(0.1),\n",
    "      nn.Conv2d(channels, channels, 3, padding = 1),\n",
    "      nn.GroupNorm(groups, channels),\n",
    "      nn.LeakyReLU(0.1),\n",
    "      nn.Conv2d(channels, channels, 1)\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "      return self.net(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetEncDec(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    dim,\n",
    "    *,\n",
    "    channels = 3,\n",
    "    layers = 4,\n",
    "    layer_mults = None,\n",
    "    num_resnet_blocks = 1,\n",
    "    resnet_groups = 16,\n",
    "    first_conv_kernel_size = 5\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    # group norm is across channels, why is this check done for dim?\n",
    "    assert dim % resnet_groups == 0, f'Dimension {dim} must divide the number of groups'\n",
    "\n",
    "    self.layers = layers\n",
    "    self.encoders = nn.ModuleList([])\n",
    "    self.decoders = nn.ModuleList([])\n",
    "\n",
    "    if layer_mults is None:\n",
    "      layer_mults = [2 ** t for t in range(layers)]\n",
    "\n",
    "    assert len(layer_mults) == layers, 'layer multipliers must be equal to number of layers'\n",
    "\n",
    "    layer_dims = [dim * mult for mult in layer_mults]\n",
    "    dims = (dim, *layer_dims)\n",
    "\n",
    "    self.encoded_dim = dims[-1]\n",
    "    \n",
    "    # (input channels, output channels) pairs for convolutions ?\n",
    "    dim_pairs = zip(dims[:-1], dims[1:])\n",
    "\n",
    "    append = lambda arr, t: arr.append(t)\n",
    "    prepend = lambda arr, t: arr.insert(0, t)\n",
    "\n",
    "    # if num_resnet_blocks is a number\n",
    "    # turns it into a tuple\n",
    "    # filling in the missing spots with 0s\n",
    "    # obtaining a tuple of length equal to the number of layers\n",
    "    # so it considers that only the last layer contains resnet blocks?\n",
    "    if not isinstance(num_resnet_blocks, tuple):\n",
    "      num_resnet_blocks = (*((0, )*(layers - 1)), num_resnet_blocks)\n",
    "\n",
    "    assert len(num_resnet_blocks) == layers, 'must  specify the number of resnet blocks for each layer'\n",
    "\n",
    "    for layer_index, (dim_in, dim_out), layer_num_resnet_blocks in zip(range(layers), dim_pairs, num_resnet_blocks):\n",
    "      append(self.encoders, nn.Sequential(nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1), nn.LeakyReLU(0.1)))\n",
    "      prepend(self.decoders, nn.Sequential(nn.ConvTranspose2d(dim_out, dim_in, 2, 1), nn.LeakyReLU(0.1)))\n",
    "\n",
    "      for _ in range(layer_num_resnet_blocks):\n",
    "        append(self.encoders, ResBlock(dim_out, groups = resnet_groups))\n",
    "        prepend(self.decoders, GLUResBlock(dim_out, groups = resnet_groups))\n",
    "\n",
    "    prepend(self.encoders, nn.Conv2d(channels, dim, first_conv_kernel_size, padding = first_conv_kernel_size // 2))\n",
    "    append(self.decoders, nn.Conv2d(dim, channels, 1))\n",
    "\n",
    "    \"\"\"\n",
    "    encoders: Conv2d(channels, dim)\n",
    "              (Conv2d(dim, dim ** 2, stride = 2), n * Resnet(dim ** 2)\n",
    "              (Conv2d(dim ** 2, dim ** 4, stride =2), n * Resnet(dim ** 4))\n",
    "              .\n",
    "              .\n",
    "              .\n",
    "              \n",
    "\n",
    "    decoders: (GLUResBlock(dim_final)\n",
    "              ConvTranspose2d(dim_final, dim_final // 2, kernel = 2))\n",
    "              .\n",
    "              .\n",
    "              .\n",
    "              .\n",
    "              (GLUResBlock(dim ** 2)\n",
    "              ConvTranspose2d(dim ** 2, dim, kernel = 2)\n",
    "              )\n",
    "              Conv2d(dim, channels)\n",
    "              \n",
    "    \"\"\"    \n",
    "\n",
    "  def get_encoded_fmap_size(self, image_size):\n",
    "    return image_size // (2 ** self.layers)\n",
    "\n",
    "  @property\n",
    "  def last_dec_layer(self):\n",
    "    return self.decoders[-1].weight\n",
    "\n",
    "  def encode(self, x):\n",
    "    for enc in self.encoders:\n",
    "      x = enc(x)\n",
    "    return x\n",
    "\n",
    "  def decode(self, x):\n",
    "    for dec in self.decoders:\n",
    "      x = dec(x)\n",
    "    return x    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_discr_loss(fake, real):\n",
    "  \"\"\"\n",
    "  fake is the output of the discriminator when its input is the decoded image\n",
    "  real is the output of the discriminator when its input is the real image\n",
    "  where decoded image = Dec(Enc(real image))\n",
    "  the discriminator is supposed to return 0 for images it considers fake\n",
    "  and 1 for images it considers real\n",
    "  the relu ensures clamps the output to be positive\n",
    "  ignoring the relu, the loss becomes\n",
    "  2 + fake - real\n",
    "  so to minimize the loss, we must have fake = 0, meaning the discriminator considers the decoded images fake\n",
    "  and we must have real = 1, which means the discriminator considers the original images real\n",
    "  \"\"\"\n",
    "  return (F.relu(1 + fake) + F.relu(1 - real)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_gen_loss(fake):\n",
    "  \"\"\"\n",
    "  The generator (which is the decoder) tries to pass its images as real\n",
    "  so it wants fake to be equal to 1, meaning the discriminator considers them real\n",
    "  which corresponds to maximizing fake, or minimizing -fake\n",
    "  \"\"\"\n",
    "  return - fake.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      dims,\n",
    "      channels = 3,\n",
    "      groups = 16,\n",
    "      init_kernel_size = 5\n",
    "  ):\n",
    "    super().__init__()\n",
    "    dim_pairs = zip(dims[:-1], dims[1:])\n",
    "\n",
    "    # project to dims[0] channels\n",
    "    self.layers = nn.ModuleList([nn.Sequential(nn.Conv2d(channels, dims[0], init_kernel_size, padding = init_kernel_size // 2), nn.LeakyReLU(0.1))])\n",
    "\n",
    "    for dim_in, dim_out in dim_pairs:\n",
    "      self.layers.append(nn.Sequential(\n",
    "        nn.Conv2d(dim_in, dim_out, 4, stride = 2, padding = 1),\n",
    "        nn.GroupNorm(groups, dim_out),\n",
    "        nn.LeakyReLU(0.1)\n",
    "      ))\n",
    "\n",
    "    dim = dims[-1]\n",
    "    self.to_logits = nn.Sequential(\n",
    "      nn.Conv2d(dim, dim, 1),\n",
    "      nn.LeakyReLU(0.1),\n",
    "      nn.Conv2d(dim, 1, 4)\n",
    "    )\n",
    "\n",
    "  # returns a 5x5 1-channel \"image\" if it receives a 32x32 image\n",
    "  def forward(self, x):\n",
    "    for net in self.layers:\n",
    "      x = net(x)\n",
    "\n",
    "    return self.to_logits(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_div(numer, denom, eps = 1e-8):\n",
    "  return numer / denom.clamp(min = eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vgg(fn):\n",
    "  \"\"\"\n",
    "  Decorator that removes the vgg attribute,\n",
    "  runs the function fn, and then reads it.\n",
    "  Used to dump and load the state dict \n",
    "  since VGG is pretrained\n",
    "  \"\"\"\n",
    "  @wraps(fn)\n",
    "  def inner(self, *args, **kwargs):\n",
    "    has_vgg = hasattr(self, '_vgg')\n",
    "    if has_vgg:\n",
    "      vgg = self._vgg\n",
    "      delattr(self, '_vgg')\n",
    "    out = fn(self, *args, **kwargs)\n",
    "    \n",
    "    if has_vgg:\n",
    "      self._vgg = vgg\n",
    "\n",
    "    return out\n",
    "\n",
    "  return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQGanVAE(nn.Module):\n",
    "  # only dim = 128 and codebook_size = 65536 were passed\n",
    "  def __init__(\n",
    "    self,\n",
    "    *,\n",
    "    dim,\n",
    "    channels = 3,\n",
    "    layers = 4,\n",
    "    vgg = None,\n",
    "    codebook_size = 65536,\n",
    "    # always True\n",
    "    use_vgg_and_gan = True,\n",
    "    discr_layers = 4,\n",
    "    **kwargs\n",
    "  ):\n",
    "    super().__init__()\n",
    "    \n",
    "    self.channels = channels\n",
    "    self.codebook_size = codebook_size\n",
    "    self.dim_divisor = 2 ** layers\n",
    "\n",
    "    enc_dec_class = ResnetEncDec\n",
    "    self.enc_dec = enc_dec_class(\n",
    "      dim = dim,\n",
    "      channels = channels,\n",
    "      layers = layers\n",
    "    )\n",
    "\n",
    "    self.quantizer = LFQ(\n",
    "      dim = self.enc_dec.encoded_dim,\n",
    "      codebook_size = codebook_size,\n",
    "      diversity_gamma = 4\n",
    "    )\n",
    "\n",
    "    # reconstruction loss\n",
    "    # mean of |target - input|\n",
    "    self.recon_loss_fn = F.l1_loss\n",
    "\n",
    "    # vgg remaind None\n",
    "    self._vgg = None\n",
    "    self.discr = None\n",
    "    self.use_vgg_and_gan = use_vgg_and_gan\n",
    "\n",
    "    layer_mults = list(map(lambda t: 2 ** t, range(discr_layers)))\n",
    "    layer_dims = [dim * mult for mult in layer_mults]\n",
    "    dims = (dim, *layer_dims)\n",
    "\n",
    "    self.discr = Discriminator(dims = dims, channels = channels)\n",
    "    \n",
    "    self.discr_loss = hinge_discr_loss\n",
    "    self.gen_loss = hinge_gen_loss\n",
    "\n",
    "  @property\n",
    "  def device(self):\n",
    "    return next(self.parameters()).device\n",
    "\n",
    "  @property\n",
    "  def vgg(self):\n",
    "    if self._vgg is not None:\n",
    "      return self._vgg\n",
    "    \n",
    "    # vgg takes in a 3-channel image and returns 4096 features\n",
    "    vgg = torchvision.models.vgg16(pretrained = True)\n",
    "    vgg.classifier = nn.Sequential(*vgg.classifier[:-2])\n",
    "    self._vgg = vgg.to(self.device)\n",
    "\n",
    "    return self._vgg\n",
    "\n",
    "  @property\n",
    "  def encoded_dim(self):\n",
    "    \"\"\"\n",
    "    The number of channels obtained after running the encoder\n",
    "    \"\"\"\n",
    "    return self.enc_dec.encoded_dim\n",
    "  \n",
    "  def get_encoded_fmap_size(self, image_size):\n",
    "    \"\"\"\n",
    "    Resolution of image after encoding\n",
    "    \"\"\"\n",
    "    return self.enc_dec.get_encoded_fmap_size(image_size)\n",
    "\n",
    "  def copy_for_eval(self):\n",
    "    \"\"\"\n",
    "    Creates a copy of the model from which it removes the discriminator\n",
    "    and the VGG, and then sets it up for evaluation\n",
    "    \"\"\"\n",
    "    device = next(self.parameters()).device\n",
    "    vae_copy = copy.deepcopy(self.cpu())\n",
    "\n",
    "    if vae_copy.use_vgg_and_gan:\n",
    "      del vae_copy.discr\n",
    "      del vae_copy._vgg\n",
    "\n",
    "    vae_copy.eval()\n",
    "    return vae_copy.to(device)\n",
    "\n",
    "  @remove_vgg\n",
    "  def load_state_dict(self, *args, **kwargs):\n",
    "    return super().state_dict(*args, **kwargs)\n",
    "\n",
    "  @remove_vgg\n",
    "  def load_state_dict(self, *args, **kwargs):\n",
    "    return super().load_state_dict(*args, **kwargs)\n",
    "\n",
    "  def save(self, path):\n",
    "    torch.save(self.state_dict(), path)\n",
    "\n",
    "  def load(self, path):\n",
    "    path = Path(path)\n",
    "    assert path.exists()\n",
    "    state_dict = torch.load(str(path))\n",
    "    self.load_state_dict(state_dict)\n",
    "\n",
    "  def encode(self, fmap):\n",
    "    fmap = self.enc_dec.encode(fmap)\n",
    "    fmap, indices, vq_aux_loss = self.quantizer(fmap)\n",
    "    return fmap, indices, vq_aux_loss\n",
    "\n",
    "  def decode(self, fmap):\n",
    "    return self.enc_dec.decode(fmap)\n",
    "\n",
    "  \n",
    "\n",
    "  def forward(\n",
    "      self,\n",
    "      img,\n",
    "      return_loss = False,\n",
    "      # whether the discriminator loss is returned\n",
    "      return_discr_loss = False,\n",
    "      # whether the reconstructed image is returned\n",
    "      return_recons = False,\n",
    "      add_gradient_penalty = True\n",
    "  ):\n",
    "    batch, channels, height, width, device = *img.shape, img.device\n",
    "\n",
    "    assert (height % self.dim_divisor) == 0, f'For the encoder, height must divide {self.dim_divisor}'\n",
    "    assert (width % self.dim_divisor) == 0, f'For the encoder, width must divide {self.dim_divisor}'\n",
    "\n",
    "    fmap, indices, commit_loss = self.encode(img)\n",
    "    fmap = self.decode(fmap)\n",
    "\n",
    "    if not return_loss and not return_discr_loss:\n",
    "      return fmap\n",
    "\n",
    "    if return_discr_loss:\n",
    "      fmap.detach_()\n",
    "      img.requires_grad_()\n",
    "\n",
    "      fmap_discr_logits, img_discr_logits = self.discr(fmap), self.discr(img)\n",
    "\n",
    "      discr_loss = self.discr_loss(fmap_discr_logits, img_discr_logits)\n",
    "\n",
    "      if add_gradient_penalty:\n",
    "        gp = gradient_penalty(img, img_discr_logits)\n",
    "        loss = discr_loss + gp\n",
    "\n",
    "      if return_recons:\n",
    "        return loss, fmap\n",
    "\n",
    "      return loss\n",
    "\n",
    "    recon_loss = self.recon_loss_fn(fmap, img)\n",
    "\n",
    "    if not self.use_vgg_and_gan:\n",
    "      if return_recons:\n",
    "        return recon_loss, fmap\n",
    "\n",
    "      return recon_loss\n",
    "    \n",
    "    img_vgg_input = img\n",
    "    fmap_vgg_input = fmap\n",
    "\n",
    "    img_vgg_feats = self.vgg(img_vgg_input)\n",
    "    recon_vgg_feats = self.vgg(fmap_vgg_input)\n",
    "    perceptual_loss = F.mse_loss(img_vgg_feats, recon_vgg_feats)\n",
    "\n",
    "    gen_loss = self.gen_loss(self.discr(fmap))\n",
    "\n",
    "    \"\"\"\n",
    "    Adaptive Loss for GANS: https://arxiv.org/pdf/2012.03149.pdf\n",
    "    Gives more or less weight to the generator loss\n",
    "    By comparing it with the perceptual loss (which can be thought of as somewhat of a discriminator loss)\n",
    "    \"\"\"\n",
    "    last_dec_layer = self.enc_dec.last_dec_layer\n",
    "\n",
    "    norm_grad_wrt_gen_loss = grad_layer_wrt_loss(gen_loss, last_dec_layer).norm(2)\n",
    "    norm_grad_wrt_perceptual_loss = grad_layer_wrt_loss(perceptual_loss, last_dec_layer).norm(2) \n",
    "\n",
    "    adaptive_weight = safe_div(norm_grad_wrt_perceptual_loss, norm_grad_wrt_gen_loss)\n",
    "    adaptive_weight.clamp_(max = 1e4)\n",
    "\n",
    "    loss = recon_loss + perceptual_loss + commit_loss + adaptive_weight * gen_loss \n",
    "\n",
    "    if return_recons:\n",
    "      return loss, fmap\n",
    "    \n",
    "    return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
