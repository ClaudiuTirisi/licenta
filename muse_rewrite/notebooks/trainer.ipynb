{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/muse/lib/python3.8/site-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from random import choice\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from vae import VQGanVAE\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from ema import EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yes_or_no(question):\n",
    "  answer = input(f'{question} (y/n)')\n",
    "  return answer.lower() in ['yes', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle(dl):\n",
    "  while True:\n",
    "    for data in dl:\n",
    "      yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accum_log(log, new_logs):\n",
    "  for key, new_value in new_logs.items():\n",
    "    old_value = log.get(key, 0.)\n",
    "    log[key] = old_value + new_value\n",
    "\n",
    "  return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "  def __init__(\n",
    "      self,\n",
    "      folder,\n",
    "      image_size,\n",
    "      exts = ['jpg', 'jpeg', 'png']\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.folder = folder\n",
    "    self.image_size = image_size\n",
    "    self.paths = [p for ext in exts for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n",
    "\n",
    "    print(f'Found {len(self.paths)} training samples at {folder}')\n",
    "\n",
    "    self.transform = T.Compose([\n",
    "      T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "      T.Resize(image_size),\n",
    "      T.RandomHorizontalFlip(),\n",
    "      T.CenterCrop(image_size),\n",
    "      T.ToTensor()\n",
    "    ])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "\n",
    "  def __get__item(self, index):\n",
    "    path = self.paths[index]\n",
    "    img = Image.open(path)\n",
    "    return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQGanVAETrainer(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      vae,\n",
    "      *,\n",
    "      folder,\n",
    "      valid_folder,\n",
    "      num_train_steps,\n",
    "      batch_size,\n",
    "      image_size,\n",
    "      lr = 3e-4,\n",
    "      grad_accum_every = 1,\n",
    "      save_results_every = 100,\n",
    "      save_model_every = 1000,\n",
    "      results_folder = \"./results\",\n",
    "      valid_frac = 0.05,\n",
    "      random_split_seed = 42,\n",
    "      use_ema = True,\n",
    "      ema_beta = 0.995,\n",
    "      ema_update_after_step = 0,\n",
    "      ema_update_every = 1,\n",
    "      apply_grad_penalty_every = 4,   \n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vae = vae\n",
    "    self.register_buffer('steps', torch.Tensor([0]))\n",
    "\n",
    "    self.accelerator = Accelerator()\n",
    "\n",
    "    self.num_train_steps = num_train_steps\n",
    "    self.batch_size = batch_size\n",
    "    self.gard_accum_every = grad_accum_every\n",
    "\n",
    "    all_parameters = set(vae.parameters())\n",
    "    discr_parameters = set(vae.discr.parameters())\n",
    "    vae_parameters = all_parameters - discr_parameters\n",
    "\n",
    "    self.vae_parameters = vae_parameters\n",
    "\n",
    "    self.optim = Adam(vae_parameters, lr = lr)\n",
    "    self.discr_optim = Adam(discr_parameters, lr = lr)\n",
    "\n",
    "    self.ds = ImageDataset(folder, image_size)\n",
    "\n",
    "    if valid_frac >0:\n",
    "      train_size = int((1 - valid_frac) * len(self.ds))\n",
    "      valid_size = len(self.ds) - train_size\n",
    "      self.ds, self.valid_ds = random_split(self.ds, [train_size, valid_size], generator = torch.Generator().manual_seed(random_split_seed))\n",
    "      print(f'Split dataset into {len(self.ds)} samples for training and {len(self.valid_ds)} samples for validating')\n",
    "    else:\n",
    "      self.valid_ds = self.ds\n",
    "      print(f'Training with shared training and validation dataset of {len(self.ds)} samples')\n",
    "\n",
    "    self.dl = DataLoader(\n",
    "      self.ds,\n",
    "      batch_size = batch_size,\n",
    "      shuffle = True\n",
    "    )\n",
    "\n",
    "    self.valid_dl = DataLoader(\n",
    "      self.valid_ds,\n",
    "      batch_size = batch_size,\n",
    "      shuffle = True\n",
    "    )\n",
    "\n",
    "    self.vae, self.optim, self.discr_optim, self.dl, self.valid_dl = self.accelerator.prepare(self.vae, self.optim, self.discr_optim, self.dl, self.valid_dl)\n",
    "\n",
    "    self.use_ema = use_ema\n",
    "    if use_ema:\n",
    "      self.ema_vae = EMA(vae, update_after_step = ema_update_after_step, update_every = ema_update_every)\n",
    "\n",
    "    self.dl_iter = cycle(self.dl)\n",
    "    self.valid_dl_iter = cycle(self.valid_dl)\n",
    "\n",
    "    self.save_model_every = save_model_every\n",
    "    self.save_results_every = save_results_every\n",
    "\n",
    "    self.apply_grad_penalty_every = apply_grad_penalty_every\n",
    "\n",
    "    self.results_folder = Path(results_folder)\n",
    "\n",
    "    if len([*self.results_folder.glob(\"**/*\")]) > 0 and yes_or_no(\"Do you want to clear previous experiment checkpoints and results?\"):\n",
    "      rmtree(str(self.results_folder))\n",
    "\n",
    "    self.results_folder.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "  def save(self, path):\n",
    "    pkg = dict(\n",
    "      model = self.accelerator.get_state_dict(self.vae),\n",
    "      optim = self.optim.state_dict(),\n",
    "      discr_optim = self.discr_optim.state_dict()\n",
    "    )\n",
    "    torch.save(pkg, path)\n",
    "\n",
    "  def load(self, path):\n",
    "    path = Path(path)\n",
    "    assert path.exists()\n",
    "    pkg = torch.load(path)\n",
    "\n",
    "    vae = self.accelerator.unwrap_model(self.vae)\n",
    "    vae.load_state_dict(pkg['model'])\n",
    "\n",
    "    self.optim.load_state_dict(pkg['optim'])\n",
    "    self.discr_optim.load_state_dict(pkg['discr_optim'])\n",
    "\n",
    "  @property\n",
    "  def device(self):\n",
    "    return self.accelerator.device\n",
    "  \n",
    "  def train_step(self):\n",
    "    device = self.device\n",
    "\n",
    "    steps = int(self.steps.item())\n",
    "    apply_grad_penalty = not (steps % self.apply_grad_penalty_every)\n",
    "\n",
    "    self.vae.train()\n",
    "    discr = self.vae.discr\n",
    "    \n",
    "    if self.use_ema:\n",
    "      ema_vae = self.ema_vae\n",
    "\n",
    "    if self.use_ema:\n",
    "      ema_vae = self.ema_vae.module\n",
    "\n",
    "    logs = {}\n",
    "    for _ in range(self.grad_accum_every):\n",
    "      img = next(self.dl_iter)\n",
    "      img = img.to(device)\n",
    "\n",
    "      with self.accelerator.autocast():\n",
    "        loss = self.vae(\n",
    "          img,\n",
    "          add_gradient_penalty = apply_grad_penalty,\n",
    "          return_loss = True\n",
    "        )\n",
    "        \n",
    "        self.accelerator.backward(loss / self.grad_accum_every)\n",
    "\n",
    "        accum_log(logs, {'loss': loss.item() / self.grad_accum_every})\n",
    "\n",
    "    self.optim.step()\n",
    "    self.optim.zero_grad()\n",
    "\n",
    "    self.discr_optim.zero_grad()\n",
    "    for _ in range(self.grad_accum_every):\n",
    "      img = next(self.dl_iter)\n",
    "      img = img.to(device)\n",
    "\n",
    "      loss = self.vae(img, return_discr_loss = True)\n",
    "      self.accelerator.backward(loss / self.grad_accum_every)\n",
    "\n",
    "      accum_log(logs, {'discr_loss': loss.item() / self.grad_accum_every})\n",
    "\n",
    "    self.discr_optim.step()\n",
    "\n",
    "    print(f\"{steps}: vae loss: {logs['loss']} - discr loss: {logs['discr_loss']}\")\n",
    "\n",
    "    if self.use_ema:\n",
    "      ema_vae.update()\n",
    "\n",
    "    if not (steps % self.save_results_every):\n",
    "      vaes_to_evaluate = ((self.vae, str(steps)),)\n",
    "\n",
    "      if self.use_ema:\n",
    "        vaes_to_evaluate = ((ema_vae.ema_model, f'{steps}.ema',)) + vaes_to_evaluate\n",
    "\n",
    "      for model, filename in vaes_to_evaluate:\n",
    "        model.eval()\n",
    "\n",
    "        valid_data = next(self.valid_dl_iter)\n",
    "        valid_data = valid_data.to(device)\n",
    "\n",
    "        recons = model(valid_data, return_recons = True)\n",
    "\n",
    "        imgs_and_recons = torch.stack((valid_data, recons), dim = 0)\n",
    "        imgs_and_recons = rearrange(imgs_and_recons, 'r b ... -> (b r) ...')\n",
    "\n",
    "        imgs_and_recons = imgs_and_recons.detach().cpu().float().clamp(0., 1.)\n",
    "        grid = make_grid(imgs_and_recons, nrow = 2, normalize = True, value_range = (0, 1))\n",
    "\n",
    "        logs['reconstructions'] = grid\n",
    "\n",
    "        save_image(grid, str(self.results_folder / f'{filename}.png'))\n",
    "\n",
    "      print(f'{steps}: saving to {str(self.results_folder)}')\n",
    "\n",
    "      self.accelerator.wait_for_everyone()\n",
    "      if self.is_main and not (steps % self.save_model_every):\n",
    "        state_dict = self.accelerator.unwrap_model(self.vae).state_dict()\n",
    "        model_path = str(self.results_folder / f'vae.{steps}.pt')\n",
    "        self.accelerator.save(state_dict, model_path)\n",
    "\n",
    "        if self.use_ema:\n",
    "          ema_state_dict = self.accelerator.unwrap_model(self.ema_vae).state_dict()\n",
    "          model_path = str(self.results_folder / f'vae.{steps}.ema.pt')\n",
    "          self.accelerator.save(ema_state_dict, model_path)\n",
    "\n",
    "        print(f'{steps}: saving model to {str(self.results_folder)}')\n",
    "\n",
    "      self.steps += 1\n",
    "      return logs\n",
    "\n",
    "  def train(self):\n",
    "    \n",
    "    while self.steps < self.num_train_steps:\n",
    "      logs = self.train_step()\n",
    "\n",
    "    print('training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"COCO-Captions/fast-ai-coco/train2017\"\n",
    "valid_path = \"COCO-Captions/fast-ai-coco/val2017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VQGanVAE(\n",
    "    dim = 128,\n",
    "    codebook_size = 65536\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 training samples at COCO-Captions/fast-ai-coco/train2017\n",
      "Split dataset into 0 samples for training and 0 samples for validating\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train on folder of images, as many images as possible\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# one training steps sees batch_size * (2*grad_accum_every) images\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# so 4*2*8=64 images\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# about 2000 training steps = 1 epoch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mVQGanVAETrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# image_size ignored if is_web_dataset is True\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalid_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_accum_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n",
      "Cell \u001b[0;32mIn[6], line 55\u001b[0m, in \u001b[0;36mVQGanVAETrainer.__init__\u001b[0;34m(self, vae, folder, valid_folder, num_train_steps, batch_size, image_size, lr, grad_accum_every, save_results_every, save_model_every, results_folder, valid_frac, random_split_seed, use_ema, ema_beta, ema_update_after_step, ema_update_every, apply_grad_penalty_every)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\n\u001b[1;32m     53\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining with shared training and validation dataset of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m  \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m  \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     59\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dl \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_ds,\n\u001b[1;32m     63\u001b[0m   batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[1;32m     64\u001b[0m   shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscr_optim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscr_optim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdl, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dl)\n",
      "File \u001b[0;32m~/anaconda3/envs/muse/lib/python3.8/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/muse/lib/python3.8/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "\n",
    "# train on folder of images, as many images as possible\n",
    "\n",
    "# one training steps sees batch_size * (2*grad_accum_every) images\n",
    "# so 4*2*8=64 images\n",
    "# about 2000 training steps = 1 epoch\n",
    "\n",
    "trainer = VQGanVAETrainer(\n",
    "    vae = vae,\n",
    "    # image_size ignored if is_web_dataset is True\n",
    "    image_size = 512,\n",
    "    folder = train_path,\n",
    "    valid_folder = valid_path,\n",
    "    batch_size = 2,\n",
    "    grad_accum_every = 4,\n",
    "    num_train_steps = 30000,\n",
    "    lr=5e-5,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
