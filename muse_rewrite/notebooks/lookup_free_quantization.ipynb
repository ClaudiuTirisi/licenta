{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2, ceil\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from einops import rearrange, reduce, pack, unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_one(t, pattern):\n",
    "  return pack([t], pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_one(t, ps, pattern):\n",
    "  return unpack(t, ps, pattern)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(t, eps = 1e-5):\n",
    "  return t.clamp(min = eps).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(prob):\n",
    "  return (-prob * log(prob)).sum(dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LFQ(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      *,\n",
    "      # encoded dim, as in, number of channels\n",
    "      dim = None,\n",
    "      # user-supplied\n",
    "      codebook_size = None,\n",
    "      entropy_loss_weight = 0.1,\n",
    "      commitment_loss_weight = 0.25,\n",
    "      # set to 4\n",
    "      diversity_gamma = 1.,\n",
    "      straight_through_activation = nn.Identity(),\n",
    "      # always equal to 1\n",
    "      num_codebooks = 1,\n",
    "      keep_num_codebooks_dim = None,\n",
    "      codebook_scale = 1.,\n",
    "      frac_per_sample_entropy = 1.\n",
    "  ):\n",
    "    super().__init__()\n",
    "    assert dim is not None or codebook_size is not None, 'either dim or codebook_size must be specified'\n",
    "    assert dim is None or log2(codebook_size).is_integer(), 'codebook size must be power of 2 for lookup free quantization'\n",
    "\n",
    "    if codebook_size is None:\n",
    "      codebook_size = 2 ** dim\n",
    "\n",
    "    codebook_dim = int(log2(codebook_size))\n",
    "\n",
    "    codebook_dims = codebook_dim * num_codebooks\n",
    "\n",
    "    if dim is None:\n",
    "      dim = codebook_dims\n",
    "\n",
    "    has_projections = dim != codebook_dims\n",
    "    self.project_in = nn.Linear(dim, codebook_dims) if has_projections else nn.Identity()\n",
    "    self.project_out = nn.Linear(codebook_dims, dim) if has_projections else nn.Identity()\n",
    "    self.has_projections = has_projections\n",
    "\n",
    "    self.dim = dim\n",
    "    self.codebook_dim = codebook_dim\n",
    "    self.num_codebooks = num_codebooks\n",
    "\n",
    "    if keep_num_codebooks_dim is None:\n",
    "      keep_num_codebooks_dim = num_codebooks > 1\n",
    "    \n",
    "    assert not (num_codebooks > 1 and not keep_num_codebooks_dim)\n",
    "\n",
    "    self.keep_num_codebooks_dim = keep_num_codebooks_dim\n",
    "\n",
    "    self.activation = straight_through_activation\n",
    "\n",
    "    assert 0 < frac_per_sample_entropy <= 1.\n",
    "    self.frac_per_sample_entropy = frac_per_sample_entropy\n",
    "\n",
    "    self.diversity_gamma = diversity_gamma\n",
    "    self.entropy_loss_weight = entropy_loss_weight\n",
    "\n",
    "    self.codebook_scale = codebook_scale\n",
    "\n",
    "    self.commitment_loss_weight = commitment_loss_weight\n",
    "\n",
    "    self.register_buffer(\"mask\", 2 ** torch.arange(codebook_dim - 1, -1, -1))\n",
    "    self.register_buffer(\"zero\", torch.tensor(0.), persistent = False)\n",
    "\n",
    "    all_codes = torch.arange(codebook_size)\n",
    "\n",
    "    # all numbers from 0 to codebook_size as bits\n",
    "    # codebook_dim is chosen to be log2(codebook_size)\n",
    "    # and bits.shape = codebook_size x codebook_dim\n",
    "    # ensuring there's enough space for all numbers from 0 to codebook_size\n",
    "    bits = ((all_codes[..., None].int() & self.mask) != 0).float()\n",
    "\n",
    "    # turns the 0 bit into -1, and keeps 1 as 1\n",
    "    codebook = self.bits_to_codes(bits)\n",
    "\n",
    "  def bits_to_codes(self, bits):\n",
    "    return bits * 2 - 1\n",
    "\n",
    "  @property\n",
    "  def dtype(self):\n",
    "    return self.codebook.dtype\n",
    "  \n",
    "  @autocast(enabled = False)\n",
    "  def forward(\n",
    "    self,\n",
    "    # (batch, channels, width, height)\n",
    "    x,\n",
    "    # unmodified\n",
    "    inv_temperature = 100.,\n",
    "    # always False\n",
    "    return_loss_breakdown = False,\n",
    "    # always None\n",
    "    mask = None\n",
    "  ):\n",
    "    x = x.float()\n",
    "    is_img_or_video = x.ndim >= 4\n",
    "\n",
    "    # moves the channels dimension to the end\n",
    "    # and flattens the spatial dimensions\n",
    "    # afterwards, x[b][i][c] is the value of channel c, for the ith pixel in image b\n",
    "    if is_img_or_video:\n",
    "      x = rearrange(x, 'b d ... -> b ... d')\n",
    "      x, ps = pack_one(x, 'b * d')\n",
    "\n",
    "    assert x.shape[-1] == self.dim, \"Incorrect number of channels passed to quantizer. Should be equal to number of channels the encoder outputs\"\n",
    "\n",
    "    # linear layer that operates on the channel dimension, that was now moved to the end\n",
    "    # and is considered the feature dimension\n",
    "    x = self.project_in(x)\n",
    "\n",
    "    # splits x along the feature dimension among the codebooks\n",
    "    # we only use one codebook, so x.shape = (b, n, 1, d)\n",
    "    # essentially just adding a dimension\n",
    "    x = rearrange(x, 'b n (c d) -> b n c d', c = self.num_codebooks)\n",
    "\n",
    "    # replace positive values with 1, and negative values with -1\n",
    "    original_input = x\n",
    "    codebook_value = torch.ones_like(x)\n",
    "    quantized = torch.where(x > 0, codebook_value, -codebook_value)\n",
    "\n",
    "    # the above \"binarization\" leads to gradients that are equal to 0\n",
    "    # when computing gradients in the backward phase we skip the gradient of the \n",
    "    # binarization function\n",
    "    if self.training:\n",
    "      x = x + (quantized - x).detach()\n",
    "    else:\n",
    "      x = quantized\n",
    "\n",
    "    # ignoring the batch, each element in the sequence (the flattened spatial dimension)\n",
    "    # is transformed into its corresponding binary number\n",
    "    # by considering values greater than 0 as a 1-bit and everything else as a 0-bit\n",
    "    indices = reduce((x > 0).int() * self.mask.int(), 'b n c d -> b n c', 'sum')\n",
    "\n",
    "    if self.training:\n",
    "\n",
    "      # distance[n][c] = -2 * sum over d of original_input[n][d] * self.codebook[c][d]\n",
    "      # distance[n][c] = -2 original_input[n] dot codebook[c]\n",
    "      distance = -2 * einsum('... i d, j d -> ... i j', original_input, self.codebook)\n",
    "      \n",
    "      # probability of each code in the codebook being chosen\n",
    "      # assuming we are choosing codes based on cosine similarity\n",
    "      # which is close to euclidean distance since the codes are\n",
    "      # equal to -1 or 1\n",
    "      prob = (-distance * inv_temperature).softmax(dim = -1)\n",
    "\n",
    "      # consolidate all samples, by flattening across batch dimension\n",
    "      prob = rearrange(prob, 'b n ... -> (b n) ...')\n",
    "\n",
    "      # entropy for each sample\n",
    "      # if high, that means the model doesn't strongly prefer one codebook entry\n",
    "      # to represent the sample\n",
    "      # if low, that means the model strongly prefers one codebook entry\n",
    "      per_sample_entropy = entropy(prob).mean()\n",
    "\n",
    "      # for each codebook entry, compute the probability of it being chosen\n",
    "      avg_prob = reduce(prob, '... c d -> c d', 'mean')\n",
    "      codebook_entropy = entropy(avg_prob).mean()\n",
    "\n",
    "      # we want the model to clearly choose one codebook entry \n",
    "      # so we minimize per sample entropy\n",
    "      # and we want the model to make use of all the codebook entries\n",
    "      # so we maximize the codebook entropy\n",
    "\n",
    "      entropy_aux_loss = per_sample_entropy - self.diversity_gamma * codebook_entropy\n",
    "      \n",
    "    else:\n",
    "      entropy_aux_loss = per_sample_entropy = codebook_entropy = self.zero\n",
    "\n",
    "    # we want the encoder to commit to a certain codebook representation\n",
    "    # by having its output already be close to quantized\n",
    "    if self.training:\n",
    "      commit_loss = F.mse_loss(original_input, quantized.detach(), reduction = 'none')\n",
    "      commit_loss = commit_loss.mean()\n",
    "    else:\n",
    "      commit_loss = self.zero\n",
    "      \n",
    "    x = rearrange(x, 'b n c d -> b n (c d)')\n",
    "    \n",
    "    x = self.project_out(x)\n",
    "\n",
    "    if is_img_or_video:\n",
    "      x = unpack_one(x, ps, 'b * d')\n",
    "      x = rearrange(x, 'b ... d -> b d ...')\n",
    "      \n",
    "    if not self.keep_num_codebooks_dim:\n",
    "      indices = rearrange(indices, '... 1 -> ...')\n",
    "\n",
    "    aux_loss = entropy_aux_loss * self.entropy_loss_weight + commit_loss * self.commitment_loss_weight\n",
    "\n",
    "    if not return_loss_breakdown:\n",
    "      return x, indices, aux_loss\n",
    "    \n",
    "    return x, indices, aux_loss, per_sample_entropy, codebook_entropy, commit_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "codebook_size = 2**13\n",
    "codebook_dim = 13\n",
    "mask = 2 ** torch.arange(codebook_dim - 1, -1, -1)\n",
    "all_codes = torch.arange(codebook_size)\n",
    "bits = ((all_codes[..., None].int() & mask) !=0).float()\n",
    "codebook = bits*2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 100, 25, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(x, 'b d ... -> b ... d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, ps = pack_one(x, 'b * d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Linear(100, 13)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(x, 'b n (c d) -> b n c d', c = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_input = x\n",
    "codebook_value = torch.ones_like(x)\n",
    "quantized = torch.where(x > 0, codebook_value, -codebook_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = reduce((x > 0).int() * mask.int(), 'b n c d -> b n c', 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = -2 * einsum('... i d, j d -> ... i j', original_input, codebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 625, 1, 13])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(x, 'b n c d -> b n (c d)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 625, 13])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Linear(13, 100)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = unpack_one(x, ps, 'b * d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 25, 100])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(x, 'b ... d -> b d ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 25, 25])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = unpack_one(indices, ps, 'b * c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 25, 25])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 25, 1])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = rearrange(indices, '... 1 -> ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25, 25])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
