{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "from mambaCText import MambaCText\n",
    "\n",
    "from muse_maskgit_pytorch import LayerNorm\n",
    "\n",
    "from mamba_ssm.ops.triton.layernorm import layer_norm_fn\n",
    "from einops import repeat\n",
    "\n",
    "from masked_model import MaskedModel, SequenceModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from muse_maskgit_pytorch import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule(t):\n",
    "    return torch.cos(t * math.pi * 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaIT_CText(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      token_size,\n",
    "      depth,\n",
    "      d_state = 16,\n",
    "      d_conv = 4,\n",
    "      expand = 2,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.token_size = token_size\n",
    "    self.mamba_layers = nn.ModuleList([MambaCText(d_model = token_size, d_state = d_state, d_conv = d_conv, expand = expand) for _ in range(depth)])\n",
    "    \n",
    "    self.attention = Attention(token_size, cross_attend = True)\n",
    "    \n",
    "    self.norm = LayerNorm(token_size)\n",
    "\n",
    "  def forward(\n",
    "      self,\n",
    "      x,\n",
    "      context,\n",
    "      context_mask\n",
    "  ):\n",
    "\n",
    "    seq_len = x.shape[1]\n",
    "\n",
    "    context_mask = repeat(context_mask, 'b t -> b t s', s = self.token_size)\n",
    "    context = torch.where(context_mask, context, torch.zeros_like(context))\n",
    "\n",
    "    context = self.attention(x, context)\n",
    "\n",
    "    print(context.shape)\n",
    "\n",
    "    for mamba_layer in self.mamba_layers:\n",
    "      x = mamba_layer(x, context)\n",
    "      x = self.norm(x)\n",
    "\n",
    "    return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
