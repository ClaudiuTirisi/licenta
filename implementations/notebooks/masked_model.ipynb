{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://github.com/lucidrains/muse-maskgit-pytorch/\n",
    "\n",
    "All models take in precomputed tokens, and not images/text, as much as possible, and various unused settings were removed. The HighRes model now has a separate embedding layer for the conditioning image tokens.\n",
    "\n",
    "Structure was modified to easily swap sequence models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_encoded_dim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_encoded_dim\u001b[49m(DEFAULT_T5_NAME)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_encoded_dim' is not defined"
     ]
    }
   ],
   "source": [
    "get_encoded_dim(DEFAULT_T5_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/muse/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from muse_maskgit_pytorch.t5 import t5_encode_text, DEFAULT_T5_NAME, get_encoded_dim\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(texts):\n",
    "  return t5_encode_text(DEFAULT_T5_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to generate a boolean tensor of a given shape where each element has a probablity `prob` of being `True`. Used for classifier free guidance, and taken from https://github.com/lucidrains/muse-maskgit-pytorch/blob/main/muse_maskgit_pytorch/muse_maskgit_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(shape, min = 0, max = 1, device = None):\n",
    "    return torch.zeros(shape, device = device).float().uniform_(0, 1)\n",
    "\n",
    "def prob_mask_like(shape, prob, device = None):\n",
    "  if prob == 1:\n",
    "    return torch.ones(shape, device = device, dtype = torch.bool)\n",
    "  if prob == 0:\n",
    "    return torch.zeros(shape, device = device, dtype = torch.bool)\n",
    "  return uniform(shape, device = device) < prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a mask tensor, and, if the tensor contains $n$ True values, then roughly `prob %` of those values will stay True and the rest will be switched to False. This corresponds to unmasking those particular tokens.\n",
    "\n",
    "From https://github.com/lucidrains/muse-maskgit-pytorch/blob/main/muse_maskgit_pytorch/muse_maskgit_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_subset_prob(mask, prob, min_mask = 0):\n",
    "    batch, seq, device = *mask.shape, mask.device\n",
    "    num_to_mask = (mask.sum(dim = -1, keepdim = True) * prob).clamp(min = min_mask)\n",
    "    logits = torch.rand((batch, seq), device = device)\n",
    "    logits = logits.masked_fill(~mask, -1)\n",
    "\n",
    "    randperm = logits.argsort(dim = -1).argsort(dim = -1).float()\n",
    "\n",
    "    num_padding = (~mask).sum(dim = -1, keepdim = True)\n",
    "    randperm -= num_padding\n",
    "\n",
    "    subset_mask = randperm < num_to_mask\n",
    "    subset_mask.masked_fill_(~mask, False)\n",
    "    return subset_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gumbel distribution. I call this softmax with noise. Also from https://github.com/lucidrains/muse-maskgit-pytorch/blob/main/muse_maskgit_pytorch/muse_maskgit_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(t, eps = 1e-20):\n",
    "  return torch.log(t.clamp(min = eps))\n",
    "\n",
    "def gumbel_noise(t):\n",
    "  \"\"\"\n",
    "  0 < noise < 1\n",
    "  clamp:\n",
    "  1e-20 < noise < 1\n",
    "  -46 < log(noise) < 0\n",
    "  0 < -log(noise) < 46\n",
    "  clamp:\n",
    "  1e-20 < -log(noise) < 46\n",
    "  -46 < log(-log(noise)) < 3.8\n",
    "  -3.8 < -log(-log(noise)) < 46\n",
    "\n",
    "  in reality around -2 < output < 4 due to slow increase of log\n",
    "\n",
    "  \"\"\"\n",
    "  noise = torch.zeros_like(t).uniform_(0,1)\n",
    "  return -log(-log(noise))\n",
    "\n",
    "def gumbel_sample(t, temperature = 1., dim = -1):\n",
    "  return ((t / max(temperature, 1e-10)) + gumbel_noise(t)).argmax(dim = dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class that implements a generic sequence model that takes in prepared image tokens and prepared text tokens (as in, passed through embedding layers and whatever else may be needed), as well as a `sequence_model` that must accept a tuple of `(image_tokens, text_tokens)` as input and return an output of the same shape as `image_tokens`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of `image_tokens` and `text_tokens` must be equal to `(batch_size, image_token_count, dim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModelWrapper(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    sequence_model,\n",
    "    sequence_model_token_size,\n",
    "    token_value_count, # how many values can an image token take\n",
    "    sequence_length, # how many tokens is each image represented with\n",
    "    is_high_resolution # whether the model is a super res model, conditioned on lower res tokens\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.sequence_model = sequence_model\n",
    "\n",
    "    # takes an input of shape (batch_size, indice_count) that contains the token values for each image\n",
    "    # and embedds it to shape (batch_size, indice_count, sequence_model_token_size)\n",
    "    # the embedding layer associates a value of size equal to sequence_model_token_size\n",
    "    # to each of the possible token values; the number of token values is +1\n",
    "    # because we have to take the MASK token into account as well\n",
    "    self.token_emb = nn.Embedding(token_value_count + 1, sequence_model_token_size)\n",
    "\n",
    "    # token values span from 0 to token_value_count - 1, so the next value\n",
    "    # is assigned to be the mask token\n",
    "    self.mask_id = token_value_count\n",
    "\n",
    "    if is_high_resolution:\n",
    "      # embedding layer for the lowres image\n",
    "      # this image doesn't require a MASK token\n",
    "      self.lowres_token_emb = nn.Embedding(token_value_count, sequence_model_token_size)\n",
    "\n",
    "    # associates a vector of size sequence_model_token_size to each position in the \n",
    "    # sequence of image tokens\n",
    "    self.pos_emb = nn.Embedding(sequence_length, sequence_model_token_size)\n",
    "\n",
    "    # the sequence model outputs the same shape sequence\n",
    "    # it represents each token as a vector of size sequence_model_token_size\n",
    "    # so we project that to size token_value_count to obtain\n",
    "    # a probability distribution of each token\n",
    "    self.to_logits = nn.Linear(sequence_model_token_size, token_value_count)\n",
    "\n",
    "    self.encode_text = encode_text\n",
    "\n",
    "    text_embed_size = get_encoded_dim(DEFAULT_T5_NAME)\n",
    "\n",
    "    # text token embed size must be equal to image token size\n",
    "    self.text_embed_proj = nn.Linear(text_embed_size, sequence_model_token_size, bias = False)\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    x, # image tokens, preflattened\n",
    "    text_embeds, # precomputed text tokens as returned by T5\n",
    "    return_embed_only, # return sequence model output before passing it through to_logits; used for self critic\n",
    "    # option image tokens generated by a lower resolution model\n",
    "    # precomputed and preflattened\n",
    "    return_loss_only,\n",
    "    conditioning_image_ids = None,\n",
    "    labels = None,\n",
    "    ignore_index = 0,\n",
    "    cond_drop_prob = 0.,\n",
    "  ):\n",
    "    device, batch_size, n = x.device, *x.shape\n",
    "    \n",
    "    context = self.text_embed_proj(text_embeds)\n",
    "\n",
    "    # a text embedding is masked if all its entries are equal to 0\n",
    "    context_mask = (text_embeds != 0).any(dim = -1)\n",
    "\n",
    "    # classifier free guidance\n",
    "    # drop the conditioning text tokens for a fraction of the mini batches\n",
    "    mask = prob_mask_like((batch_size, 1), 1 - cond_drop_prob, device)\n",
    "    context_mask = context_mask & mask\n",
    "\n",
    "    if conditioning_image_ids is not None:\n",
    "      # passed through the same embedding as the main image sequence\n",
    "      cond_token_emb = self.lowres_token_emb(conditioning_image_ids)\n",
    "\n",
    "      # concatenate the 2 conditioning sequencing\n",
    "      # resulting in a longer sequence\n",
    "      context = torch.cat((context, cond_token_emb), dim = -2)\n",
    "      \n",
    "      # pad the context mask with True for the newly added conditioning tokens\n",
    "      context_mask = F.pad(context_mask, (0, conditioning_image_ids.shape[-1]), value = True)\n",
    "\n",
    "    x = self.token_emb(x)\n",
    "    x = x + self.pos_emb(torch.arange(n, device = device))\n",
    "\n",
    "    embed = self.sequence_model(x, context = context, context_mask = context_mask)\n",
    "\n",
    "    if return_embed_only:\n",
    "      return embed\n",
    "    \n",
    "    logits = self.to_logits(embed)\n",
    "\n",
    "    if labels is None:\n",
    "      return embed, logits\n",
    "\n",
    "    if self.dim_out == 1:\n",
    "      # loss for self-token-critic\n",
    "      loss = F.binary_cross_entropy_with_logits(rearrange(logits, '... 1 -> ...'), labels)\n",
    "    else:\n",
    "      # loss for normal model\n",
    "      loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), labels, ignore_index = ignore_index)\n",
    "\n",
    "    if return_loss_only:\n",
    "      return loss\n",
    "\n",
    "    return logits\n",
    "\n",
    "  def forward_with_cond_scale(\n",
    "    self,\n",
    "    x, # image tokens, preflattened\n",
    "    text_embeds, # precomputed text tokens as returned by T5\n",
    "    return_embed_only, # return sequence model output before passing it through to_logits; used for self critic\n",
    "    return_loss_only,\n",
    "    # option image tokens generated by a lower resolution model\n",
    "    # precomputed and preflattened\n",
    "    conditioning_image_ids = None,\n",
    "    labels = None,\n",
    "    ignore_index = 0,\n",
    "    cond_drop_prob = 0.,\n",
    "    cond_scale = 3.,\n",
    "  ):\n",
    "    args = [text_embeds, return_embed_only, return_loss_only]\n",
    "    kw_args = dict(conditioning_image_ids = conditioning_image_ids, labels = labels, ignore_index = ignore_index, cond_drop_prob = cond_drop_prob)\n",
    "\n",
    "    if cond_scale == 1:\n",
    "      return self.forward(x, *args, **kw_args, cond_drop_prob = 0.)\n",
    "    \n",
    "    logits, embed = self.forward(x, *args, **kw_args, cond_drop_prob = 1., return_embed = True)\n",
    "\n",
    "    null_logits = self.forward(x, *args, cond_drop_prob = 1., **kw_args)\n",
    "\n",
    "    return null_logits + (logits - null_logits) * cond_scale, embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaskedModel takes in image tokens, masks them according to a noise schedule and passes them to a sequence model wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenCritic(nn.Module):\n",
    "  def __init__(\n",
    "      self, net\n",
    "  ):\n",
    "    self.net = net\n",
    "    self.to_pred = nn.Linear(net.sequence_model_token_size)\n",
    "\n",
    "  def forward(self, x, *args, labels = None, **kwargs):\n",
    "    embeds = self.net.forward_with_cond_scale(x, *args, return_only_embed = True, **kwargs)\n",
    "    logits = self.to_pred(embeds)\n",
    "\n",
    "    logits = rearrange(logits, \"... 1 -> ...\")\n",
    "    return F.binary_cross_entropy_with_logits(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedModel(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    sequence_model,\n",
    "    noise_schedule,\n",
    "    no_mask_token_prob = 0.,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.sequence_model = sequence_model\n",
    "    self.mask_id = sequence_model.mask_id\n",
    "    self.noise_schedule = noise_schedule\n",
    "\n",
    "    self.token_critic = TokenCritic(self.sequence_model)\n",
    "\n",
    "    # probability for some of the masked tokens to be unmasked\n",
    "    self.no_mask_token_prob = no_mask_token_prob\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    image_ids, # assumed to already be flattened\n",
    "    ignore_index = -1,\n",
    "    conditioning_token_ids = None, # assumed to already be flattened\n",
    "    text_embeds = None,\n",
    "    cond_drop_prob = None,\n",
    "  ):\n",
    "    batch, seq_len, device = *image_ids.shape, image_ids.device\n",
    "\n",
    "    # pick a random time for the noise scheduler\n",
    "    # leading to a random number of tokens to be masked\n",
    "    # each mini batch (each image) gets its own mask probability\n",
    "    rand_time = uniform((batch, ), device = device)\n",
    "    rand_mask_probs = self.noise_schedule(rand_time)\n",
    "    num_token_masked = (seq_len * rand_mask_probs).round().clamp(min = 1)\n",
    "\n",
    "    mask_id = self.mask_id\n",
    "\n",
    "    # random permutation of the tokens\n",
    "    # without this permutation, the first tokens in the sequence would always be masked\n",
    "    # and the last tokens would almost never be masked\n",
    "    batch_randperm = torch.rand((batch, seq_len), device = device).argsort(dim = -1)\n",
    "    mask = batch_randperm < rearrange(num_token_masked, 'b -> b 1')\n",
    "\n",
    "    # when computing the loss, we only care about the loss resulting from\n",
    "    # the masked tokens (that the sequence model must unmask)\n",
    "    # hence we mark the unmasked position with ignore_index\n",
    "    # which the cross entropy loss will know to ignore \n",
    "    labels = torch.where(mask, image_ids, ignore_index)\n",
    "\n",
    "    if self.no_mask_token_prob > 0.:\n",
    "      no_mask_mask = get_mask_subset_prob(mask, self.no_mask_token_prob)\n",
    "      # the function get_mask_subset_prob keeps no_mask_token_prob % of the tokens as True\n",
    "      # those tokens are no longer masked because True & !True = False\n",
    "      mask &= ~no_mask_mask\n",
    "\n",
    "    x = torch.where(mask, mask_id, image_ids)\n",
    "\n",
    "    ce_loss, logits = self.sequence_model.forward_with_cond_scale(\n",
    "      x,\n",
    "      text_embeds = text_embeds,\n",
    "      conditioning_token_ids = conditioning_token_ids,\n",
    "      labels = labels,\n",
    "      cond_drop_prob = cond_drop_prob,\n",
    "      ignore_index = ignore_index,\n",
    "      return_logits = True\n",
    "    )\n",
    "\n",
    "    if self.token_critic is None:\n",
    "      return ce_loss\n",
    "    \n",
    "    # normally we would apply softmax to obtain the predicted token value\n",
    "    # however for training the token critic, a noisy softmax is used to choose\n",
    "    # the predicted values\n",
    "    sampled_ids = gumbel_sample(logits, temperature = random())\n",
    "    \n",
    "    # the masked tokens are unmasked, the rest stay correct\n",
    "    critic_input = torch.where(mask, sampled_ids, x)\n",
    "\n",
    "    # True if predicted tokens matched ground truth, false otherwise\n",
    "    critic_labels = (image_ids != critic_input).float()\n",
    "\n",
    "    # token critic is passed the predicted tokens\n",
    "    # and compares them to the groundtruth in critic_labels\n",
    "    bce_loss = self.token_critic(\n",
    "      critic_input,\n",
    "      text_embeds = text_embeds,\n",
    "      conditioning_token_ids = conditioning_token_ids,\n",
    "      labels = critic_labels,\n",
    "      cond_drop_prob = cond_drop_prob\n",
    "    )\n",
    "\n",
    "    return ce_loss + bce_loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
