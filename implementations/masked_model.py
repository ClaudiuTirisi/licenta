import sys
sys.path.append("..")

import torch
import torch.nn as nn

import torch.nn.functional as F

from random import random

from muse_maskgit_pytorch.t5 import t5_encode_text, DEFAULT_T5_NAME, get_encoded_dim

from einops import rearrange

from tqdm import tqdm

import math

def encode_text(texts):
  return t5_encode_text(texts, DEFAULT_T5_NAME)

def uniform(shape, min = 0, max = 1, device = None):
    return torch.zeros(shape, device = device).float().uniform_(0, 1)

def prob_mask_like(shape, prob, device = None):
  if prob == 1:
    return torch.ones(shape, device = device, dtype = torch.bool)
  if prob == 0:
    return torch.zeros(shape, device = device, dtype = torch.bool)
  return uniform(shape, device = device) < prob

def get_mask_subset_prob(mask, prob, min_mask = 0):
    batch, seq, device = *mask.shape, mask.device
    num_to_mask = (mask.sum(dim = -1, keepdim = True) * prob).clamp(min = min_mask)
    logits = torch.rand((batch, seq), device = device)
    logits = logits.masked_fill(~mask, -1)

    randperm = logits.argsort(dim = -1).argsort(dim = -1).float()

    num_padding = (~mask).sum(dim = -1, keepdim = True)
    randperm -= num_padding

    subset_mask = randperm < num_to_mask
    subset_mask.masked_fill_(~mask, False)
    return subset_mask

def log(t, eps = 1e-20):
  return torch.log(t.clamp(min = eps))

def gumbel_noise(t):
  """
  0 < noise < 1
  clamp:
  1e-20 < noise < 1
  -46 < log(noise) < 0
  0 < -log(noise) < 46
  clamp:
  1e-20 < -log(noise) < 46
  -46 < log(-log(noise)) < 3.8
  -3.8 < -log(-log(noise)) < 46

  in reality around -2 < output < 4 due to slow increase of log

  """
  noise = torch.zeros_like(t).uniform_(0,1)
  return -log(-log(noise))

def gumbel_sample(t, temperature = 1., dim = -1):
  return ((t / max(temperature, 1e-10)) + gumbel_noise(t)).argmax(dim = dim) 

class SequenceModelWrapper(nn.Module):
  def __init__(
    self,
    sequence_model,
    token_value_count, # how many values can an image token take
    sequence_length, # how many tokens is each image represented with
    is_high_resolution # whether the model is a super res model, conditioned on lower res tokens
  ):
    super().__init__()
    self.sequence_model = sequence_model

    self.sequence_model_token_size = sequence_model.token_size

    # takes an input of shape (batch_size, indice_count) that contains the token values for each image
    # and embedds it to shape (batch_size, indice_count, sequence_model_token_size)
    # the embedding layer associates a value of size equal to sequence_model_token_size
    # to each of the possible token values; the number of token values is +1
    # because we have to take the MASK token into account as well
    self.token_emb = nn.Embedding(token_value_count + 1, self.sequence_model_token_size)

    # token values span from 0 to token_value_count - 1, so the next value
    # is assigned to be the mask token
    self.mask_id = token_value_count

    if is_high_resolution:
      # embedding layer for the lowres image
      # this image doesn't require a MASK token
      self.lowres_token_emb = nn.Embedding(token_value_count, self.sequence_model_token_size)

    # associates a vector of size sequence_model_token_size to each position in the 
    # sequence of image tokens
    self.pos_emb = nn.Embedding(sequence_length, self.sequence_model_token_size)

    # the sequence model outputs the same shape sequence
    # it represents each token as a vector of size sequence_model_token_size
    # so we project that to size token_value_count to obtain
    # a probability distribution of each token
    self.to_logits = nn.Linear(self.sequence_model_token_size, token_value_count)

    self.encode_text = encode_text

    text_embed_size = get_encoded_dim(DEFAULT_T5_NAME)

    # text token embed size must be equal to image token size
    self.text_embed_proj = nn.Linear(text_embed_size, self.sequence_model_token_size, bias = False)

  def forward(
    self,
    x, # image tokens, preflattened
    text_embeds, # precomputed text tokens as returned by T5
    return_embed_only = False, # return sequence model output before passing it through to_logits; used for self critic
    # optional image tokens generated by a lower resolution model
    # precomputed and preflattened
    conditioning_token_ids = None,
    cond_drop_prob = 0.,
  ):
    device, batch_size, n = x.device, *x.shape
    
    context = self.text_embed_proj(text_embeds)

    # a text embedding is masked if all its entries are equal to 0
    context_mask = (text_embeds != 0).any(dim = -1)

    # classifier free guidance
    # drop the conditioning text tokens for a fraction of the mini batches
    mask = prob_mask_like((batch_size, 1), 1 - cond_drop_prob, device)
    context_mask = context_mask & mask

    if conditioning_token_ids is not None:
      # passed through the same embedding as the main image sequence
      cond_token_emb = self.lowres_token_emb(conditioning_token_ids)

      # concatenate the 2 conditioning sequencing
      # resulting in a longer sequence
      context = torch.cat((context, cond_token_emb), dim = -2)
      
      # pad the context mask with True for the newly added conditioning tokens
      context_mask = F.pad(context_mask, (0, conditioning_token_ids.shape[-1]), value = True)

    x = self.token_emb(x)

    x = x + self.pos_emb(torch.arange(n, device = device))

    embed = self.sequence_model(x, context = context, context_mask = context_mask)

    if return_embed_only:
      return None, embed
    
    logits = self.to_logits(embed)
    
    return logits, embed

  def forward_with_cond_scale(
    self,
    x, # image tokens, preflattened
    text_embeds, # precomputed text tokens as returned by T5
    return_embed_only = False, # return sequence model output before passing it through to_logits; used for self critic
    return_loss_only = False,
    # option image tokens generated by a lower resolution model
    # precomputed and preflattened
    conditioning_token_ids = None,
    ignore_index = 0,
    cond_scale = 3.,
  ):

    if cond_scale == 1:
      return self.forward(
        x, text_embeds, return_embed_only = return_embed_only, cond_drop_prob = 0.,
        conditioning_token_ids = conditioning_token_ids, ignore_index = ignore_index
        )

    logits, embed = self.forward(
      x, text_embeds, return_embed_only = return_embed_only, cond_drop_prob = 0.,
      conditioning_token_ids = conditioning_token_ids, 
    )

    null_logits, _ = self.forward(
      x, text_embeds, return_embed_only = return_embed_only, cond_drop_prob = 1.,
      conditioning_token_ids = conditioning_token_ids, 
    )

    return null_logits + (logits - null_logits) * cond_scale, embed

class TokenCritic(nn.Module):
  def __init__(
      self, net
  ):
    super().__init__()
    self.net = net
    self.to_pred = nn.Linear(net.sequence_model_token_size, 1)

  def forward(
      self, 
      critic_input, 
      text_embeds,
      conditioning_token_ids,
      cond_scale
    ):
    _, embed = self.net.forward_with_cond_scale(
      critic_input,
      text_embeds,
      conditioning_token_ids = conditioning_token_ids,
      cond_scale = cond_scale
      )
    logits = self.to_pred(embed)

    logits = rearrange(logits, "... 1 -> ...")
    return logits, embed

class MaskedModel(nn.Module):
  def __init__(
    self,
    sequence_model,
    noise_schedule,
    no_mask_token_prob = 0.,
    cond_drop_prob = 0.5,
    cond_scale = 3,
  ):
    super().__init__()

    self.sequence_model = sequence_model
    self.mask_id = sequence_model.mask_id
    self.noise_schedule = noise_schedule

    self.token_critic = TokenCritic(self.sequence_model)

    # probability for some of the masked tokens to be unmasked
    self.no_mask_token_prob = no_mask_token_prob

    self.cond_drop_prob = cond_drop_prob
    self.cond_scale = cond_scale

  def forward(
    self,
    image_ids, # assumed to already be flattened
    ignore_index = -1,
    conditioning_token_ids = None, # assumed to already be flattened
    text_embeds = None,
    texts = None
  ):
    batch, seq_len, device = *image_ids.shape, image_ids.device

    if text_embeds is None:
      text_embeds = encode_text(texts)

    # pick a random time for the noise scheduler
    # leading to a random number of tokens to be masked
    # each mini batch (each image) gets its own mask probability
    rand_time = uniform((batch, ), device = device)
    rand_mask_probs = self.noise_schedule(rand_time)
    num_token_masked = (seq_len * rand_mask_probs).round().clamp(min = 1)

    mask_id = self.mask_id

    # random permutation of the tokens
    # without this permutation, the first tokens in the sequence would always be masked
    # and the last tokens would almost never be masked
    batch_randperm = torch.rand((batch, seq_len), device = device).argsort(dim = -1)
    mask = batch_randperm < rearrange(num_token_masked, 'b -> b 1')

    # when computing the loss, we only care about the loss resulting from
    # the masked tokens (that the sequence model must unmask)
    # hence we mark the unmasked position with ignore_index
    # which the cross entropy loss will know to ignore 
    labels = torch.where(mask, image_ids, ignore_index)

    if self.no_mask_token_prob > 0.:
      no_mask_mask = get_mask_subset_prob(mask, self.no_mask_token_prob)
      # the function get_mask_subset_prob keeps no_mask_token_prob % of the tokens as True
      # those tokens are no longer masked because True & !True = False
      mask &= ~no_mask_mask

    x = torch.where(mask, mask_id, image_ids)

    logits, _ = self.sequence_model(
      x,
      text_embeds = text_embeds,
      conditioning_token_ids = conditioning_token_ids,
      cond_drop_prob = self.cond_drop_prob,
    )

    ce_loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), labels, ignore_index = ignore_index)
    
    if self.token_critic is None:
      return ce_loss
    
    # normally we would apply softmax to obtain the predicted token value
    # however for training the token critic, a noisy softmax is used to choose
    # the predicted values
    sampled_ids = gumbel_sample(logits, temperature = random())
    
    # the masked tokens are unmasked, the rest stay correct
    critic_input = torch.where(mask, sampled_ids, x)

    # True if predicted tokens matched ground truth, false otherwise
    critic_labels = (image_ids != critic_input).float()

    # token critic is passed the predicted tokens
    # and compares them to the groundtruth in critic_labels
    bce_logits, _ = self.token_critic(
      critic_input,
      text_embeds = text_embeds,
      conditioning_token_ids = conditioning_token_ids,
      cond_scale = self.cond_scale
    )

    bce_loss = F.binary_cross_entropy_with_logits(bce_logits, critic_labels)

    return ce_loss + bce_loss    
  
  def generate(
    self,
    texts,
    fmap_size,
    conditioning_token_ids = None,
    force_not_use_token_critic = False,
    timesteps = 18,
    cond_scale = 3.,
    critic_noise_scale = 1,
    topk_filter_thres = 0.9
  ):
    device = next(self.parameters()).device
    seq_len = fmap_size ** 2

    text_embeds = encode_text(texts)

    batch_size = len(text_embeds)
    shape = (batch_size, seq_len)

    ids = torch.full(shape, self.mask_id, dtype = torch.long, device = device)
    scores = torch.zeros(shape, dtype = torch.float32, device = device)

    starting_temperature = 1
    
    for timestep, steps_until_x0 in tqdm(zip(torch.linspace(0, 1, timesteps, device = device), reversed(range(timesteps))), total = timesteps):
      rand_mask_prob =  self.noise_schedule(timestep)
      num_token_masked = max(int((rand_mask_prob * seq_len).item()), 1)

      masked_indices = scores.topk(num_token_masked, dim = -1).indices

      ids = ids.scatter(1, masked_indices, self.mask_id)

      logits, embed = self.sequence_model.forward_with_cond_scale(
        ids,
        text_embeds = text_embeds,
        conditioning_token_ids = conditioning_token_ids,
        cond_scale = cond_scale
      )

      filtered_logits = top_k(logits, topk_filter_thres)
      temperature = starting_temperature * (steps_until_x0 / timesteps)

      pred_ids = gumbel_sample(filtered_logits, temperature = temperature, dim = -1)

      is_mask = ids == self.mask_id

      ids = torch.where(
        is_mask,
        pred_ids,
        ids
      )

      if not force_not_use_token_critic:
        scores, _ = self.token_critic(
          ids,
          text_embeds = text_embeds,
          conditioning_token_ids = conditioning_token_ids,
          cond_scale = cond_scale
        )

        scores = scores + (uniform(scores.shape, device = device) - 0.5) * critic_noise_scale * (steps_until_x0 / timesteps)

      else:
        probs_without_temp = logits.softmax(dim = -1)
        scores = 1 - probs_without_temp.gather(2, pred_ids[..., None])
        scores = rearrange(scores, '... 1 -> ...')

        scores = scores.masked_fill(~is_mask, -1e5)

    ids = rearrange(ids, 'b (i j) -> b i j', i = fmap_size, j = fmap_size)

    return ids

def top_k(logits, thres = 0.9):
    k = math.ceil((1 - thres) * logits.shape[-1])
    val, ind = logits.topk(k, dim = -1)
    probs = torch.full_like(logits, float('-inf'))
    probs.scatter_(2, ind, val)
    return probs
