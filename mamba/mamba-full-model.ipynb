{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MambaConfig:\n",
    "\n",
    "    d_model: int = 2560\n",
    "    n_layer: int = 64\n",
    "    vocab_size: int = 50277\n",
    "    ssm_cfg: dict = field(default_factory=dict)\n",
    "    rms_norm: bool = True\n",
    "    residual_in_fp32: bool = True\n",
    "    fused_add_norm: bool = True\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    tie_embeddings: bool = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "import json\n",
    "import os\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from mamba_ssm.modules.mamba_simple import Mamba, Block\n",
    "from mamba_ssm.utils.generation import GenerationMixin\n",
    "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_block(\n",
    "    d_model,\n",
    "    ssm_cfg = None,\n",
    "    norm_epsilon = 1e-5,\n",
    "    rms_norm = False,\n",
    "    residual_in_fp32 = False,\n",
    "    fused_add_norm = False,\n",
    "    layer_idx = None,\n",
    "    device = None,\n",
    "    dtype = None,\n",
    "):\n",
    "  if ssm_cfg is None:\n",
    "    ssm_cfg = {}\n",
    "\n",
    "  factory_kwargs = {\n",
    "    \"device\": device,\n",
    "    \"dtype\": dtype\n",
    "  }\n",
    "\n",
    "  mixer_cls = partial(Mamba, layer_idx = layer_idx, **ssm_cfg, **factory_kwargs)\n",
    "\n",
    "  norm_cls = partial(\n",
    "    nn.LayerNorm if not rms_norm else RMSNorm, eps = norm_epsilon, **factory_kwargs\n",
    "  )\n",
    "\n",
    "  block = Block(\n",
    "    d_model,\n",
    "    mixer_cls,\n",
    "    norm_cls = norm_cls,\n",
    "    fused_add_norm = fused_add_norm,\n",
    "    residual_in_fp32 = residual_in_fp32,\n",
    "  )\n",
    "\n",
    "  block.layer_idx = layer_idx\n",
    "  return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
    "def _init_weights(\n",
    "    module,\n",
    "    n_layer,\n",
    "    initializer_range=0.02,  # Now only used for embedding layer.\n",
    "    rescale_prenorm_residual=True,\n",
    "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
    "):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        if module.bias is not None:\n",
    "            if not getattr(module.bias, \"_no_reinit\", False):\n",
    "                nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        nn.init.normal_(module.weight, std=initializer_range)\n",
    "\n",
    "    if rescale_prenorm_residual:\n",
    "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
    "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
    "        #   > the weights of residual layers at initialization by a factor of 1/âˆšN where N is the # of residual layers.\n",
    "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
    "        #\n",
    "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
    "        for name, p in module.named_parameters():\n",
    "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
    "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
    "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
    "                # We need to reinit p since this code could be called multiple times\n",
    "                # Having just p *= scale would repeatedly scale it down\n",
    "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
    "                with torch.no_grad():\n",
    "                    p /= math.sqrt(n_residuals_per_layer * n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerModel(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      d_model,\n",
    "      n_layer,\n",
    "      vocab_size,\n",
    "      ssm_cfg = None,\n",
    "      norm_epsilon = 1e-5,\n",
    "      rms_norm = False,\n",
    "      initializer_cfg = None,\n",
    "      fused_add_norm = False,\n",
    "      residual_in_fp32 = False,\n",
    "      device = None,\n",
    "      dtype = None\n",
    "  ):\n",
    "    factory_kwargs = {\n",
    "      \"device\": device,\n",
    "      \"dtype\": dtype\n",
    "    }\n",
    "    super().__init__()\n",
    "    self.residual_in_fp32 = residual_in_fp32\n",
    "    \n",
    "    self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
    "\n",
    "    self.fused_add_norm = fused_add_norm\n",
    "    if self.fused_add_norm:\n",
    "      if layer_norm_fn is None or rms_norm_fn is None:\n",
    "        raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
    "\n",
    "    self.layers = nn.ModuleList(\n",
    "      [\n",
    "        create_block(\n",
    "          d_model,\n",
    "          ssm_cfg = ssm_cfg,\n",
    "          norm_epsilon = norm_epsilon,\n",
    "          rms_norm = rms_norm,\n",
    "          residual_in_fp32 = residual_in_fp32,\n",
    "          fused_add_norm = fused_add_norm,\n",
    "          layer_idx = i,\n",
    "          **factory_kwargs\n",
    "        ) for i in range(n_layer)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(d_model, eps = norm_epsilon, **factory_kwargs)\n",
    "\n",
    "    self.apply(\n",
    "      partial(\n",
    "        _init_weights,\n",
    "        n_layer = n_layer,\n",
    "        **(initializer_cfg if initializer_cfg is not None else {})\n",
    "      )\n",
    "    )\n",
    "\n",
    "\n",
    "  def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "      return {\n",
    "          i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
    "          for i, layer in enumerate(self.layers)\n",
    "      }\n",
    "\n",
    "  def forward(self, input_ids, inference_params = None):\n",
    "    hidden_states = self.embedding(input_ids)\n",
    "    residual = None\n",
    "    for layer in self.layers:\n",
    "      hidden_states, residual = layer(\n",
    "        hidden_states, residual, inference_params = inference_params\n",
    "      )\n",
    "\n",
    "    if not self.fused_add_norm:\n",
    "      residual = (hidden_states + residual) if residual is not None else hidden_states\n",
    "\n",
    "      hidden_states = self.norm_f(residual.to(dtype = self.norm_f.weight.dtype))\n",
    "\n",
    "    else:\n",
    "      fused_add_norm_fn = rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn\n",
    "\n",
    "      hidden_states = fused_add_norm_fn(\n",
    "        hidden_states,\n",
    "        self.norm_f.weight,\n",
    "        self.norm_f.bias,\n",
    "        eps = self.norm_f.eps,\n",
    "        residual = residual,\n",
    "        prenorm = False,\n",
    "        residual_in_fp32 = self.residual_in_fp32\n",
    "      )\n",
    "\n",
    "    return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
