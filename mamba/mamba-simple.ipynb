{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "from torch.cuda.amp import custom_bwd, custom_fwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
    "import causal_conv1d_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selective_scan_cuda\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm.ops.triton.selective_state_update import selective_state_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_model = dimension of input (size of a sequence element)\n",
    "# d_state = SSM state expansion factor\n",
    "# d_conv = convolution width (filter size?)\n",
    "# expand = block expansion factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length, dim = 1, 64, 16\n",
    "x = torch.randn(batch, length, dim).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nn.Conv1d(10, 10, 1).weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaInnerFn(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  @custom_fwd\n",
    "  def forward(\n",
    "    ctx, xz,\n",
    "    conv1d_weight, conv1d_bias, x_proj_weight,\n",
    "    delta_proj_weight, out_proj_weight, out_proj_bias,\n",
    "    A, B = None, C = None, D = None, delta_bias = None,\n",
    "    B_proj_bias = None, C_proj_bias = None, \n",
    "    delta_softplus = True, checkpoint_lvl = 1\n",
    "  ):\n",
    "    \"\"\"\n",
    "    xz: (batch, dim, seqlen), (batch, 2*input_dim*expand, seqlen)\n",
    "    \"\"\"\n",
    "    assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d\"\n",
    "    assert checkpoint_lvl in [0, 1]\n",
    "    L = xz.shape[-1]\n",
    "    # delta_proj_weight is d_inner x dt_rank\n",
    "    delta_rank = delta_proj_weight.shape[1]\n",
    "    \n",
    "    # A is d_inner x d_state\n",
    "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "\n",
    "    if torch.is_autocast_enabled():\n",
    "      x_proj_weight = x_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "      delta_proj_weight = delta_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "      out_proj_weight = out_proj_weight.to(dtype=torch.get_autocast_gpu_dtype())\n",
    "      out_proj_bias = (out_proj_bias.to(dtype=torch.get_autocast_gpu_dtype()) if out_proj_bias is not None else None)\n",
    "\n",
    "    # how could xz.stride(-1) possible be unequal to 1?\n",
    "    if xz.stride(-1) != 1:\n",
    "      xz = xz.contiguous()\n",
    "\n",
    "    # d = d_inner and w = convolution filter size\n",
    "    # second dimension is equal to 1 because groups = in_channels = out_channels\n",
    "    # d = d_inner = d_model * expand\n",
    "    conv1d_weight = rearrange(conv1d_weight, \"d 1 w -> d w\")\n",
    "\n",
    "    # split across sequence element dimension\n",
    "    # possible since in_proj send d_model to d_inner * 2\n",
    "    x, z = xz.chunk(2, dim = 1)\n",
    "\n",
    "    if conv1d_bias is not None:\n",
    "      conv1d_bias = conv1d_bias.contiguous()\n",
    "\n",
    "    # parameters of causal_conv1d_fwd are:\n",
    "    # x - input sequence\n",
    "    # conv1dweight, conv1dbias - weight and bias\n",
    "    # seq_idx - perhaps it can output only on the seq element?\n",
    "    # initial_states - no idea\n",
    "    # final_states_out = no idea\n",
    "    # activation - boolean, presumably whether to apply an activation function or not\n",
    "    # conv1d_out should be (batch, inner_dim, seq_len)\n",
    "    conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(\n",
    "      x, conv1d_weight, conv1d_bias, None, None, None, True\n",
    "    )\n",
    "\n",
    "    # collapse batch and sequence dimensions into a single full sequence dimension\n",
    "    # and apply the x_proj linear layer\n",
    "    # result is of shape (seq, self.dt_rank + self.d_state * 2)\n",
    "    x_dbl = F.linear(rearrange(conv1d_out, 'b d l -> (b l) d'), x_proj_weight)\n",
    "\n",
    "    # take the first dt_rank columns from x_dbl\n",
    "    # and project them to d_inner\n",
    "    # obtaining delta of shape (seq, d_inner)\n",
    "    # finally uncompresses the batch x seq_len dim\n",
    "    # obtaining output of shape (batch, seq, d_inner)\n",
    "    delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(), \"d (b l) -> b d l\", l = L)\n",
    "\n",
    "    ctx.is_variable_B = B is None\n",
    "    ctx.is_variable_C = C is None\n",
    "    ctx.B_proj_bias_is_None = B_proj_bias is None\n",
    "    ctx.C_proj_bias_is_None = C_proj_bias is None\n",
    "\n",
    "    # if B wasn't passed to forward, that means it is input dependent\n",
    "    # so we pick it from x_dbl\n",
    "    if B is None:\n",
    "      B = x_dbl[:, delta_rank:delta_rank + d_state]\n",
    "      if B_proj_bias is not None:\n",
    "        B = B + B_proj_bias.to(dtype = B.dtype)\n",
    "\n",
    "      # if A is complex, then B must also be complex\n",
    "      # a complex number is represented via 2 real numbers\n",
    "      # which results in splitting the dstate dimension into 2\n",
    "\n",
    "      if not A.is_complex():\n",
    "        # why 1?\n",
    "        B = rearrange(B, \"(b l) dstate -> b 1 dstate l\", l = L).contiguous()\n",
    "      else:\n",
    "        B = rearrange(B, \"(b l) (dstate two) -> b 1 dstate (l two)\",\n",
    "                      l = L, two = 2).contiguous()\n",
    "\n",
    "    else:\n",
    "      if B.stride(-1) != 1:\n",
    "        B = B.contiguous()\n",
    "   \n",
    "    if C is None:\n",
    "      C = x_dbl[:, -d_state:]\n",
    "      if C_proj_bias is not None:\n",
    "        C = C + C_proj_bias.to(dtype = C.dtype)\n",
    "      if not A.is_complex():\n",
    "        C = rearrange(C, \"(b l) dstate -> b 1 dstate l\", l = L).contiguous()\n",
    "      else:\n",
    "        C = rearrange(C, \"(b l) (dstate two) -> b 1 dstate (l two)\", l = L, two = 2).contiguous()\n",
    "\n",
    "    else:\n",
    "      if C.stride(-1) != 1:\n",
    "        C = C.contiguous()\n",
    "\n",
    "    if D is not None:\n",
    "      D = D.contiguous()\n",
    "\n",
    "    # mamba_block = linear(ssm(sigma(conv(first_half)))*sigma(second_half))\n",
    "    # out is the output of SSM left side, and out_z is the output of before \n",
    "    # multiplication (?)\n",
    "\n",
    "    out, scan_intermediates, out_z = selective_scan_cuda.fwd(\n",
    "      conv1d_out,\n",
    "      delta,\n",
    "      A, B, C, D,\n",
    "      z,\n",
    "      delta_bias,\n",
    "      delta_softplus\n",
    "    )\n",
    "\n",
    "    ctx.delta_softplus = delta_softplus\n",
    "    ctx.out_proj_bias_is_None = out_proj_bias is None\n",
    "    ctx.checkpoint_lvl = checkpoint_lvl\n",
    "\n",
    "    # recompute conv1d_out and delta in backward pass\n",
    "    if checkpoint_lvl >= 1:\n",
    "      conv1d_out, delta = None, None\n",
    "\n",
    "    ctx.save_for_backward(\n",
    "      xz,\n",
    "      conv1d_weight,\n",
    "      conv1d_bias,\n",
    "      x_dbl,\n",
    "      x_proj_weight,\n",
    "      delta_proj_weight,\n",
    "      out_proj_weight,\n",
    "      conv1d_out,\n",
    "      delta,\n",
    "      A, B, C, D,\n",
    "      delta_bias,\n",
    "      scan_intermediates,\n",
    "      out\n",
    "    )\n",
    "\n",
    "    return F.linear(rearrange(out_z, \"b d l -> b l d\"), out_proj_weight, out_proj_bias)\n",
    "\n",
    "  @staticmethod\n",
    "  @custom_bwd\n",
    "  def backward(ctx, dout):\n",
    "    # dout: (batch, seqlen, dim)\n",
    "    # presumably the output of the forward pass\n",
    "    assert causal_conv1d_cuda is not None, \"causal_conv1d_cuda is not available. Please install causal-conv1d.\"\n",
    "    (xz, conv1d_weight, conv1d_bias, x_dbl, x_proj_weight, delta_proj_weight, out_proj_weight,\n",
    "      conv1d_out, delta, A, B, C, D, delta_bias, scan_intermediates, out) = ctx.saved_tensors\n",
    "    L = xz.shape[-1]\n",
    "    delta_rank = delta_proj_weight.shape[1]\n",
    "    d_state = A.shape[-1] * (1 if not A.is_complex() else 2)\n",
    "    x, z = xz.chunk(2, dim=1)\n",
    "    if dout.stride(-1) != 1:\n",
    "        dout = dout.contiguous()\n",
    "    if ctx.checkpoint_lvl == 1:\n",
    "        conv1d_out = causal_conv1d_cuda.causal_conv1d_fwd(\n",
    "            x, conv1d_weight, conv1d_bias, None, None, None, True\n",
    "        )\n",
    "        delta = rearrange(delta_proj_weight @ x_dbl[:, :delta_rank].t(),\n",
    "                          \"d (b l) -> b d l\", l = L)\n",
    "    # The kernel supports passing in a pre-allocated dz (e.g., in case we want to fuse the\n",
    "    # backward of selective_scan_cuda with the backward of chunk).\n",
    "    dxz = torch.empty_like(xz)  # (batch, dim, seqlen)\n",
    "    dx, dz = dxz.chunk(2, dim=1)\n",
    "    dout = rearrange(dout, \"b l e -> e (b l)\")\n",
    "    dout_y = rearrange(out_proj_weight.t() @ dout, \"d (b l) -> b d l\", l=L)\n",
    "    dconv1d_out, ddelta, dA, dB, dC, dD, ddelta_bias, dz, out_z = selective_scan_cuda.bwd(\n",
    "        conv1d_out, delta, A, B, C, D, z, delta_bias, dout_y, scan_intermediates, out, dz,\n",
    "        ctx.delta_softplus,\n",
    "        True  # option to recompute out_z\n",
    "    )\n",
    "    dout_proj_weight = torch.einsum(\"eB,dB->ed\", dout, rearrange(out_z, \"b d l -> d (b l)\"))\n",
    "    dout_proj_bias = dout.sum(dim=(0, 1)) if not ctx.out_proj_bias_is_None else None\n",
    "    dD = dD if D is not None else None\n",
    "    dx_dbl = torch.empty_like(x_dbl)\n",
    "    dB_proj_bias = None\n",
    "    if ctx.is_variable_B:\n",
    "        if not A.is_complex():\n",
    "            dB = rearrange(dB, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "        else:\n",
    "            dB = rearrange(dB, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "        dB_proj_bias = dB.sum(0) if not ctx.B_proj_bias_is_None else None\n",
    "        dx_dbl[:, delta_rank:delta_rank + d_state] = dB  # (bl d)\n",
    "        dB = None\n",
    "    dC_proj_bias = None\n",
    "    if ctx.is_variable_C:\n",
    "        if not A.is_complex():\n",
    "            dC = rearrange(dC, \"b 1 dstate l -> (b l) dstate\").contiguous()\n",
    "        else:\n",
    "            dC = rearrange(dC, \"b 1 dstate (l two) -> (b l) (dstate two)\", two=2).contiguous()\n",
    "        dC_proj_bias = dC.sum(0) if not ctx.C_proj_bias_is_None else None\n",
    "        dx_dbl[:, -d_state:] = dC  # (bl d)\n",
    "        dC = None\n",
    "    ddelta = rearrange(ddelta, \"b d l -> d (b l)\")\n",
    "    ddelta_proj_weight = torch.einsum(\"dB,Br->dr\", ddelta, x_dbl[:, :delta_rank])\n",
    "    dx_dbl[:, :delta_rank] = torch.einsum(\"dB,dr->Br\", ddelta, delta_proj_weight)\n",
    "    dconv1d_out = rearrange(dconv1d_out, \"b d l -> d (b l)\")\n",
    "    dx_proj_weight = torch.einsum(\"Br,Bd->rd\", dx_dbl, rearrange(conv1d_out, \"b d l -> (b l) d\"))\n",
    "    dconv1d_out = torch.addmm(dconv1d_out, x_proj_weight.t(), dx_dbl.t(), out=dconv1d_out)\n",
    "    dconv1d_out = rearrange(dconv1d_out, \"d (b l) -> b d l\", b=x.shape[0], l=x.shape[-1])\n",
    "    # The kernel supports passing in a pre-allocated dx (e.g., in case we want to fuse the\n",
    "    # backward of conv1d with the backward of chunk).\n",
    "    dx, dconv1d_weight, dconv1d_bias, *_ = causal_conv1d_cuda.causal_conv1d_bwd(\n",
    "        x, conv1d_weight, conv1d_bias, dconv1d_out, None, None, None, dx, False, True\n",
    "    )\n",
    "    dconv1d_bias = dconv1d_bias if conv1d_bias is not None else None\n",
    "    dconv1d_weight = rearrange(dconv1d_weight, \"d w -> d 1 w\")\n",
    "    return (dxz, dconv1d_weight, dconv1d_bias, dx_proj_weight, ddelta_proj_weight,\n",
    "            dout_proj_weight, dout_proj_bias,\n",
    "            dA, dB, dC, dD,\n",
    "            ddelta_bias if delta_bias is not None else None,\n",
    "            dB_proj_bias, dC_proj_bias, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mamba_inner_fn(\n",
    "    xz,\n",
    "    conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, B = None, C = None, D = None,\n",
    "    delta_bias = None, \n",
    "    B_proj_bias = None,\n",
    "    C_proj_bias = None,\n",
    "    delta_softplus = True\n",
    "):\n",
    "  return MambaInnerFn.apply(\n",
    "    xz,\n",
    "    conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
    "    out_proj_weight, out_proj_bias,\n",
    "    A, B, C, D,\n",
    "    delta_bias, B_proj_bias,\n",
    "    C_proj_bias, delta_softplus = True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mamba(nn.Module):\n",
    "  def __init__(\n",
    "      self,\n",
    "      # input size will be (batch, seq_len, d_model)\n",
    "      d_model,\n",
    "      d_state = 16,\n",
    "      d_conv = 4,\n",
    "      expand = 2,\n",
    "      dt_rank = \"auto\",\n",
    "      dt_min = 0.001,\n",
    "      dt_max = 0.1,\n",
    "      dt_init = \"random\",\n",
    "      dt_scale = 1.0,\n",
    "      dt_init_floor = 1e-4,\n",
    "      conv_bias = True,\n",
    "      bias = False,\n",
    "      use_fast_path = True,\n",
    "      layer_idx = None,\n",
    "      device = None,\n",
    "      dtype = None\n",
    "  ):\n",
    "    factory_kwargs = {\n",
    "      \"device\": device,\n",
    "      \"dtype\": dtype,\n",
    "    }\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.d_state = d_state\n",
    "    self.d_conv = d_conv\n",
    "\n",
    "    # initial sequence element length is multiplied by self.expand\n",
    "    self.expand = expand\n",
    "    self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "    # rank is d_model/16, which is 1 for the used dimensions\n",
    "    # or the supplied value (presumably an integer) if one is supplied\n",
    "    self.dt_rank = math.ceil(self.d_model / 16) if dt_rank == \"auto\" else dt_rank\n",
    "\n",
    "    self.use_fast_path = use_fast_path\n",
    "    self.layer_idx = layer_idx\n",
    "\n",
    "    # shape becomes batch x seq_len x (initial dim * expand * 2)\n",
    "    self.in_proj = nn.Linear(self.d_model, self.d_inner * 2, bias = bias, **factory_kwargs)\n",
    "\n",
    "    # input channels equal to d_inner\n",
    "    # d_inner may be the final dimension of \n",
    "    # our input, so will it be transposed?\n",
    "    self.conv1d = nn.Conv1d(\n",
    "      in_channels = self.d_inner,\n",
    "      out_channels = self.d_inner,\n",
    "      bias = conv_bias,\n",
    "      kernel_size = d_conv,\n",
    "      groups = self.d_inner,\n",
    "      padding = d_conv -1,\n",
    "      **factory_kwargs\n",
    "    )\n",
    "\n",
    "    self.activation = \"silu\"\n",
    "    self.act = nn.SiLU()\n",
    "\n",
    "    # from output of conv1d\n",
    "    # to (dt_rank + d_state * 2)\n",
    "    # presumably this is the actual input to the ssm layer\n",
    "    self.x_proj = nn.Linear(\n",
    "      self.d_inner, self.dt_rank + self.d_state * 2, bias = False,\n",
    "      **factory_kwargs\n",
    "    )\n",
    "\n",
    "    # from something of size (*, dt_rank) to something of size (*, d_inner)\n",
    "    # so far nothing appears to be of size (*, dt_rank)\n",
    "    # will the output of x_proj be split to obtain an input for this layer?\n",
    "    self.dt_proj = nn.Linear(\n",
    "      self.dt_rank,\n",
    "      self.d_inner,\n",
    "      bias = True,\n",
    "      **factory_kwargs\n",
    "    )\n",
    "\n",
    "    # initialze special dt projection to preserve variance at initialization ??\n",
    "    # init_std = scale / sqrt(dt_rank)\n",
    "    # if vectors a and b of size d have mean 0 and variance 1\n",
    "    # then their dot product has variance equal to their size (d)\n",
    "    # therefore std equal to sqrt(d)\n",
    "    # is rank equal to the size of something?\n",
    "    dt_init_std = self.dt_rank ** -0.5 * dt_scale\n",
    "\n",
    "    if dt_init == \"constant\":\n",
    "      nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "    elif dt_init == \"random\":\n",
    "      nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "    else:\n",
    "      raise NotImplementedError\n",
    "\n",
    "    # F.softplus(dt_bias) should be between dt_min and dt_max, apparently\n",
    "    # softplus = 1/B*log(1+exp(B*x))\n",
    "    # torch.rand is a uniform variable on [0, 1]\n",
    "    # [0, math.log(dt_max) - math.log(dt_min)] + math.log(dt_min)\n",
    "    # [log(min), log(max)]\n",
    "    # dt is then uniform between min and max\n",
    "    # this should be the output of softplus\n",
    "    dt = torch.exp(\n",
    "      torch.rand(self.d_inner, **factory_kwargs) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "    ).clamp(min = dt_init_floor)\n",
    "\n",
    "    # inverse of softplus\n",
    "    # y=1/B*log(1+exp(B*x))\n",
    "    # assuming B=1, y=log(1+exp(x))\n",
    "    # 1+exp(x)=exp(y)\n",
    "    # exp(x)=exp(y)-1\n",
    "    # x=log(exp(y)-1)\n",
    "    # x=log(exp(y)(1-1/exp(y)))\n",
    "    # x=log(exp(y))+log(1-1/exp(y))\n",
    "    # x=y+log(1-exp(-y))\n",
    "    # x=y+log(-(exp(-y)-1))\n",
    "    # x=y+log(-expm1(-y))\n",
    "    inv_dt = dt + torch.log(-torch.expm1(-dt))\n",
    "\n",
    "    # bias initialized to inv_dt\n",
    "    with torch.no_grad():\n",
    "      self.dt_proj.bias.copy_(inv_dt)\n",
    "\n",
    "    # presumably .init.function will ignore this parameter\n",
    "    self.dt_proj.bias._no_reinit = True\n",
    "\n",
    "    # https://arxiv.org/pdf/2206.11893.pdf\n",
    "    # each row of A is equal to [1, ... d_state]\n",
    "    A = repeat(\n",
    "      torch.arange(1, self.d_state + 1, dtype = torch.float32,\n",
    "      device = device),\n",
    "      \"n -> d n\",\n",
    "      d = self.d_inner\n",
    "    ).contiguous()\n",
    "\n",
    "    A_log = torch.log(A)\n",
    "    self.A_log = nn.Parameter(A_log)\n",
    "    self.A_log._no_weight_decay = True\n",
    "\n",
    "    self.D = nn.Parameter(torch.ones(self.d_inner, device = device))\n",
    "\n",
    "    self.D._no_weight_decay = True\n",
    "\n",
    "    self.out_proj = nn.Linear(self.d_inner, self.d_model, bias = bias, **factory_kwargs)\n",
    "\n",
    "  def _get_state_from_cache(self, inference_params, batch_size, initialize_states = False):\n",
    "    assert self.layer_idx is not None\n",
    "    if self.layer_idx not in inference_params.key_value_memory_dict:\n",
    "      batch_shape = (batch_size, )\n",
    "      conv_state = torch.zeros(\n",
    "        batch_size,\n",
    "        self.d_model * self.expand,\n",
    "        self.d_conv,\n",
    "        device = self.conv1d.weight.device,\n",
    "        dtype = self.conv1d.weight.dtype\n",
    "      )\n",
    "      ssm_state = torch.zeros(\n",
    "        batch_size,\n",
    "        self.d_model * self.expand,\n",
    "        self.d_state,\n",
    "        device = self.dt_proj.weight.device,\n",
    "        dtype = self.dt_proj.weight.device,\n",
    "      )\n",
    "      inference_params.key_value_memory_dict[self.layer_idx] = (conv_state, ssm_state)\n",
    "    else:\n",
    "      conv_state, ssm_state = inference_params.key_value_memory_dict[self.layer_idx]\n",
    "\n",
    "      if initialize_states:\n",
    "        conv_state.zero_()\n",
    "        ssm_state.zero_()\n",
    "    \n",
    "    return conv_state, ssm_state\n",
    "\n",
    "  def step(self, hidden_states, conv_state, ssm_state):\n",
    "    dtype = hidden_states.dtype\n",
    "    assert hidden_states.shape[1] == 1, \"only support decoding with 1 token at a time for now\"\n",
    "    xz = self.in_proj(hidden_states.squeeze(1))\n",
    "    x, z = xz.chunk(2, dim = -1)\n",
    "\n",
    "    x = causal_conv1d_update(\n",
    "      x,\n",
    "      conv_state,\n",
    "      rearrange(self.conv1d.weight, \"d 1 w -> d w\"),\n",
    "      self.conv1d.bias,\n",
    "      self.activation\n",
    "    )\n",
    "\n",
    "    x_db = self.x_proj(x)\n",
    "    dt, B, C = torch.split(x_db, [self.dt_rank, self.d_state, self.d_state], dim = -1)\n",
    "\n",
    "    dt = F.linear(dt, self.dt_proj.weight)\n",
    "    A = -torch.exp(self.A_log.float())\n",
    "\n",
    "    y = selective_state_update(\n",
    "      ssm_state, x, dt, A, B, C, self.D, z = z, dt_bias = self.dt_proj.bias, dt_softplus = True\n",
    "    )\n",
    "\n",
    "    out = self.out_proj(y)\n",
    "    return out.unsqueeze(1), conv_state, ssm_state\n",
    "\n",
    "\n",
    "  def forward(self, hidden_states, inference_params = None):\n",
    "    \"\"\"\n",
    "    hidden_states: (B, L, D)\n",
    "    this appears to be the actual input x\n",
    "    inference_params: ?\n",
    "    returns same shape as hidden_states\n",
    "    \"\"\"\n",
    "    batch, seqlen, dim = hidden_states.shape\n",
    "\n",
    "    conv_state, ssm_state = None, None\n",
    "    if inference_params is not None:\n",
    "      conv_state, ssm_state = self._get_states_from_cache(inference_params, batch)\n",
    "      if inference_params.seqlen_offset > 0:\n",
    "        out, _, _ = self.step(hidden_states, conv_state, ssm_state)\n",
    "        return out\n",
    "\n",
    "    # in_proj projects to 2*model dim\n",
    "    # combine batch and seq_len, transpose, linear layer\n",
    "    # and revert everything\n",
    "    # output is (batch, 2*expand*dim, seq_len)\n",
    "    xz = rearrange(\n",
    "      # flatten along batch dimension (gathering all sequences into one big sequence)\n",
    "      # and transpose so it starts with the element size dimension\n",
    "      # output is [d_inner, d_model] * [d_model, full_seq_len]\n",
    "      # which is [d_inner, full_seq_len]\n",
    "      self.in_proj.weight @ rearrange(hidden_states, \"b l d -> d (b l)\"),\n",
    "      \"d (b l) -> b d l\",\n",
    "      l = seqlen\n",
    "    )\n",
    "\n",
    "    if self.in_proj.bias is not None:\n",
    "      xz = xz + rearrange(self.in_proj.bias.to(dtype=xz.dtype), \"d -> d 1\")\n",
    "\n",
    "    # (d_inner, d_state)\n",
    "    # A[i][j]=-j\n",
    "    # S4D-Real initialization\n",
    "    A = -torch.exp(self.A_log.float())\n",
    "\n",
    "    if self.use_fast_path and causal_conv1d_fn is not None and inference_params is None:\n",
    "      out = mamba_inner_fn(\n",
    "        xz,\n",
    "        self.conv1d.weight,\n",
    "        self.conv1d.bias,\n",
    "        self.x_proj.weight,\n",
    "        self.dt_proj.weight,\n",
    "        self.out_proj.weight,\n",
    "        self.out_proj.bias,\n",
    "        A,\n",
    "        None,\n",
    "        None,\n",
    "        self.D.float(),\n",
    "        delta_bias = self.dt_proj.bias.float(),\n",
    "        delta_softplus = True\n",
    "      )\n",
    "    else:\n",
    "      x, z = xz.chunk(2, dim = 1)\n",
    "      if conv_state is not None:\n",
    "        conv_state.copy_(F.pad(x, (self.d_conv - x.shape[-1], 0)))\n",
    "\n",
    "      x = causal_conv1d_fn(\n",
    "        x = x,\n",
    "        weight = rearrange(self.weight, \"d 1 w -> d w\"),\n",
    "        bias = self.conv1d.bias,\n",
    "        activation = self.activation\n",
    "      )\n",
    "\n",
    "      x_dbl = self.x_proj(rearrange(x, \"b d l -> (b l d)\")) # (bl d)\n",
    "      dt, B, C = torch.split(x_dbl, [self.dt_rank, self.d_state, self.d_state], dim = -1)\n",
    "\n",
    "      dt = self.dt_proj.weight @ dt.t()\n",
    "      dt = rearrange(dt, \"d (b l) -> b d l\", l = seqlen)\n",
    "\n",
    "      B = rearrange(B, \"(b l) dstate -> b dstate l\", l = seqlen).contiguous()\n",
    "      C = rearrange(C, \"(b l) dstate -> b dstate l\", l = seqlen).contiguous()\n",
    "\n",
    "      y = selective_scan_fn(\n",
    "        x,\n",
    "        dt,\n",
    "        A,\n",
    "        B,\n",
    "        C,\n",
    "        self.D.float(),\n",
    "        z = z,\n",
    "        delta_bias = self.dt_proj.bias.float(),\n",
    "        delta_softplus = True,\n",
    "        return_last_state = ssm_state is not None\n",
    "      )\n",
    "\n",
    "      if ssm_state is not None:\n",
    "        y, last_state = y\n",
    "        ssm_state.copy_(last_state)\n",
    "\n",
    "      y = rearrange(y, \"b d l -> b l d\")\n",
    "      out = self.out_proj(y)\n",
    "    return out\n",
    "\n",
    "  def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "    device = self.out_proj.weight.device\n",
    "    conv_dtype = self.conv1d.weight.dtype if dtype is None else dtype\n",
    "    conv_state = torch.zeros(\n",
    "        batch_size, self.d_model * self.expand, self.d_conv, device=device, dtype=conv_dtype\n",
    "    )\n",
    "    ssm_dtype = self.dt_proj.weight.dtype if dtype is None else dtype\n",
    "    # ssm_dtype = torch.float32\n",
    "    ssm_state = torch.zeros(\n",
    "        batch_size, self.d_model * self.expand, self.d_state, device=device, dtype=ssm_dtype\n",
    "    )\n",
    "    return conv_state, ssm_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length, dim = 2, 64, 16\n",
    "model = Mamba(\n",
    "  d_model = dim,\n",
    "  d_state = 16,\n",
    "  d_conv = 4,\n",
    "  expand = 2\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(batch, length, dim).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, mixer_cls, norm_cls=nn.LayerNorm, fused_add_norm=False, residual_in_fp32=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Simple block wrapping a mixer class with LayerNorm/RMSNorm and residual connection\"\n",
    "\n",
    "        This Block has a slightly different structure compared to a regular\n",
    "        prenorm Transformer block.\n",
    "        The standard block is: LN -> MHA/MLP -> Add.\n",
    "        [Ref: https://arxiv.org/abs/2002.04745]\n",
    "        Here we have: Add -> LN -> Mixer, returning both\n",
    "        the hidden_states (output of the mixer) and the residual.\n",
    "        This is purely for performance reasons, as we can fuse add and LayerNorm.\n",
    "        The residual needs to be provided (except for the very first block).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual_in_fp32 = residual_in_fp32\n",
    "        self.fused_add_norm = fused_add_norm\n",
    "        self.mixer = mixer_cls(dim)\n",
    "        self.norm = norm_cls(dim)\n",
    "        if self.fused_add_norm:\n",
    "            assert RMSNorm is not None, \"RMSNorm import fails\"\n",
    "            assert isinstance(\n",
    "                self.norm, (nn.LayerNorm, RMSNorm)\n",
    "            ), \"Only LayerNorm and RMSNorm are supported for fused_add_norm\"\n",
    "\n",
    "    def forward(\n",
    "        self, hidden_states: Tensor, residual: Optional[Tensor] = None, inference_params=None\n",
    "    ):\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: the sequence to the encoder layer (required).\n",
    "            residual: hidden_states = Mixer(LN(residual))\n",
    "        \"\"\"\n",
    "        if not self.fused_add_norm:\n",
    "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
    "            hidden_states = self.norm(residual.to(dtype=self.norm.weight.dtype))\n",
    "            if self.residual_in_fp32:\n",
    "                residual = residual.to(torch.float32)\n",
    "        else:\n",
    "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm, RMSNorm) else layer_norm_fn\n",
    "            hidden_states, residual = fused_add_norm_fn(\n",
    "                hidden_states,\n",
    "                self.norm.weight,\n",
    "                self.norm.bias,\n",
    "                residual=residual,\n",
    "                prenorm=True,\n",
    "                residual_in_fp32=self.residual_in_fp32,\n",
    "                eps=self.norm.eps,\n",
    "            )\n",
    "        hidden_states = self.mixer(hidden_states, inference_params=inference_params)\n",
    "        return hidden_states, residual\n",
    "\n",
    "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
    "        return self.mixer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "muse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
